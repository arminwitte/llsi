{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llsi Documentation","text":"<p>Welcome to the documentation for llsi (Lightweight Linear System Identification).</p>"},{"location":"#overview","title":"Overview","text":"<p>llsi is a Python library for identifying linear time-invariant (LTI) systems from data. It provides a simple, unified interface for various system identification algorithms.</p> <p>Currently implemented methods:</p> <ul> <li>Subspace Identification: <code>n4sid</code>, <code>po-moesp</code></li> <li>Polynomial Models: <code>arx</code> (AutoRegressive with eXogenous input)</li> <li>Optimization: <code>pem</code> (Prediction Error Method), <code>oe</code> (Output Error)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install llsi\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import llsi\nimport numpy as np\n\n# 1. Prepare Data\ndata = llsi.SysIdData(t=np.arange(100), u=np.random.randn(100), y=np.random.randn(100))\n\n# 2. Identify Model\nmod = llsi.sysid(data, 'y', 'u', method='n4sid')\n\n# 3. Validate\nwith llsi.Figure() as fig:\n    fig.plot(mod)\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Tutorials: Step-by-step guides for using <code>llsi</code>.</li> <li>API Reference: Detailed documentation of classes and functions.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>For more interactive examples, check out the notebooks in the repository.</p>"},{"location":"api/figure/","title":"Plotting","text":""},{"location":"api/figure/#llsi.figure.Figure","title":"<code>llsi.figure.Figure</code>","text":"<p>Context manager for creating subplots of system identification results.</p> Source code in <code>src/llsi/figure.py</code> <pre><code>class Figure:\n    \"\"\"\n    Context manager for creating subplots of system identification results.\n    \"\"\"\n\n    def __init__(self, figsize: Tuple[int, int] = (16, 9)):\n        \"\"\"\n        Initialize the Figure context manager.\n\n        Args:\n            figsize: Tuple of (width, height) for the figure.\n        \"\"\"\n        if plt is None:\n            raise ImportError(\"matplotlib is required for plotting. Install it with 'pip install llsi[plot]'.\")\n\n        self.objects: List[Any] = []\n        self.plot_types: List[Optional[str]] = []\n        self.place: List[int] = []\n\n        self.registry = {\n            \"impulse\": self._impulse,\n            \"step\": self._step,\n            \"frequency\": self._frequency,\n            \"hsv\": self._hsv,\n            \"time_series\": self._time_series,\n            \"compare\": self._compare,\n            \"residuals\": self._residuals_acf,\n            \"residuals_acf\": self._residuals_acf,\n            \"residuals_ccf\": self._residuals_ccf,\n        }\n\n        self.figsize = figsize\n        self.counter = 0\n        self.colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n        self.logger = logging.getLogger(__name__)\n        self.fig: Optional[MplFigure] = None\n        self.ax: Optional[Union[Axes, np.ndarray]] = None\n\n    def __enter__(self) -&gt; \"Figure\":\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        if exc_type is not None:\n            return False  # Propagate exception\n\n        rows = int(np.floor((self.counter + 1) / 2))\n        cols = 1 if self.counter &lt; 2 else 2\n\n        # Handle case where no plots were added\n        if rows == 0:\n            return\n\n        self.fig, self.ax = plt.subplots(rows, cols, figsize=self.figsize, constrained_layout=True)\n\n        # Ensure self.ax is always indexable for consistency if possible,\n        # but matplotlib returns Axes or array of Axes.\n\n        for i in range(len(self.objects)):\n            plot_type = self.plot_types[i]\n            obj = self.objects[i]\n            ind = self.place[i]\n\n            # handle default cases\n            if not plot_type:\n                # deduce type\n                if isinstance(obj, SysIdData):\n                    fun = self.registry[\"time_series\"]\n                elif isinstance(obj, LTIModel):\n                    fun = self.registry[\"impulse\"]\n                elif isinstance(obj, AutoIdentResult):\n                    fun = self.registry[\"impulse\"]  # Default to impulse for result\n                else:\n                    self.logger.warning(\n                        f\"for an object of type {type(obj)} a plot_type has to be specified explicitly!\"\n                    )\n                    continue\n            else:\n                fun = self.registry.get(plot_type)\n                if fun is None:\n                    self.logger.warning(f\"Unknown plot_type '{plot_type}'. Skipping.\")\n                    continue\n\n            # handle indexing of axes with different array sizes\n            if self.counter == 1:\n                ax = self.ax\n            elif self.counter == 2:\n                ax = self.ax[ind]\n            else:\n                # For 2 columns, row = ind // 2, col = ind % 2\n                ax = self.ax[ind // 2, ind % 2]\n\n            # Determine color based on object identity or index\n            # This logic is a bit fragile but kept from original\n            try:\n                # Find index of this object in the list of unique objects added so far?\n                # Or just use the index in the current list?\n                # Original code: color_index = self.objects.index(obj)\n                # This finds the *first* occurrence.\n                color_index = self.objects.index(obj) % len(self.colors)\n            except ValueError:\n                color_index = 0\n\n            # call plotting method\n            fun(self.fig, ax, obj, col=self.colors[color_index])\n\n        # Only show the figure if matplotlib interactive mode is enabled.\n        # In test environments (Agg backend) this avoids a non-interactive warning.\n        try:\n            if plt.isinteractive():\n                plt.show()\n        except Exception:\n            # If anything unexpected occurs, avoid raising during normal program exit.\n            pass\n\n    def plot(self, obj: Union[Any, List[Any]], plot_type: Optional[str] = None):\n        \"\"\"\n        Add an object to be plotted.\n\n        Args:\n            obj: The object(s) to plot (LTIModel, SysIdData, etc.)\n            plot_type: The type of plot ('impulse', 'step', 'frequency', 'hsv', 'time_series', 'compare').\n                       If None, inferred from object type.\n        \"\"\"\n        if not isinstance(obj, (list, tuple)):\n            obj_list = [obj]\n        else:\n            obj_list = obj\n\n        if plot_type == \"residuals\":\n            # Special case: residuals implies both ACF and CCF\n            # We add two plots\n            for o in obj_list:\n                self.objects.append(o)\n                self.plot_types.append(\"residuals_acf\")\n                self.place.append(self.counter)\n                self.counter += 1\n\n                self.objects.append(o)\n                self.plot_types.append(\"residuals_ccf\")\n                self.place.append(self.counter)\n                self.counter += 1\n            return\n\n        for o in obj_list:\n            self.objects.append(o)\n            self.plot_types.append(plot_type)\n            self.place.append(self.counter)\n\n        self.counter += 1\n\n    @staticmethod\n    def _impulse(fig: MplFigure, ax: Axes, lti_mod: Union[LTIModel, AutoIdentResult], col: str = \"#1f77b4\"):\n        if isinstance(lti_mod, AutoIdentResult):\n            lti_mod = lti_mod.model\n\n        if isinstance(lti_mod, LTIModel):\n            res = lti_mod.impulse_response(N=200, uncertainty=True)\n            if len(res) == 3:\n                t, y, y_std = res\n            else:\n                t, y = res\n                y_std = None\n\n            markerline, stemlines, baseline = ax.stem(t, y)\n            plt.setp(stemlines, \"color\", col)\n            plt.setp(markerline, \"color\", col)\n\n            if y_std is not None:\n                y = y.ravel()\n                y_std = y_std.ravel()\n                # Plot confidence region around y=0 (significance band)\n                ax.fill_between(t, -2 * y_std, 2 * y_std, color=col, alpha=0.2)\n\n            ax.set_title(\"Impulse response\")\n            ax.grid(True, alpha=0.3)\n\n    @staticmethod\n    def _step(fig: MplFigure, ax: Axes, lti_mod: Union[LTIModel, AutoIdentResult], col: str = \"#1f77b4\"):\n        if isinstance(lti_mod, AutoIdentResult):\n            lti_mod = lti_mod.model\n\n        if isinstance(lti_mod, LTIModel):\n            res = lti_mod.step_response(N=200, uncertainty=True)\n            if len(res) == 3:\n                t, y, y_std = res\n            else:\n                t, y = res\n                y_std = None\n\n            ax.step(t, y, where=\"post\", color=col)\n\n            if y_std is not None:\n                y = y.ravel()\n                y_std = y_std.ravel()\n                try:\n                    ax.fill_between(t, y - 2 * y_std, y + 2 * y_std, color=col, alpha=0.2, step=\"post\")\n                except (AttributeError, TypeError):\n                    ax.fill_between(t, y - 2 * y_std, y + 2 * y_std, color=col, alpha=0.2)\n\n            ax.set_title(\"Step response\")\n            ax.grid(True, alpha=0.3)\n\n    @staticmethod\n    def _frequency(fig: MplFigure, ax: Axes, lti_mod: Union[LTIModel, AutoIdentResult], col: str = \"#1f77b4\"):\n        if isinstance(lti_mod, AutoIdentResult):\n            lti_mod = lti_mod.model\n\n        if isinstance(lti_mod, LTIModel):\n            res = lti_mod.frequency_response(uncertainty=True)\n            if len(res) == 4:\n                omega, H, mag_std, phase_std = res\n            else:\n                omega, H = res\n                mag_std = None\n                phase_std = None\n\n            H = H.squeeze()\n            mag = np.abs(H.ravel())\n            phase = np.angle(H.ravel())\n\n            ax2 = ax.twinx()\n\n            ax.plot(omega.ravel(), mag, color=col, label=\"Magnitude\")\n\n            if mag_std is not None:\n                mag_std = mag_std.ravel()\n                ax.fill_between(omega.ravel(), mag - 2 * mag_std, mag + 2 * mag_std, color=col, alpha=0.2)\n\n            ax.set_ylabel(\"Magnitude\")\n            ax.set_xlabel(\"Frequency [rad/s]\")\n            ax.set_title(\"Frequency response\")\n            ax.grid(True, alpha=0.3)\n\n            ax2.plot(omega, phase, linestyle=\"dashed\", color=col, alpha=0.6, label=\"Phase\")\n\n            if phase_std is not None:\n                phase_std = phase_std.ravel()\n                ax2.fill_between(omega.ravel(), phase - 2 * phase_std, phase + 2 * phase_std, color=col, alpha=0.1)\n\n            ax2.set_ylabel(\"Phase [rad]\")\n            ax2.set_yticks([-np.pi, -np.pi / 2, 0, np.pi / 2, np.pi])\n            ax2.set_yticklabels([r\"$-\\pi$\", r\"$-\\pi/2$\", \"0\", r\"$\\pi/2$\", r\"$\\pi$\"])\n\n    @staticmethod\n    def _hsv(fig: MplFigure, ax: Axes, ss_mod: Union[StateSpaceModel, AutoIdentResult], col: str = \"#1f77b4\"):\n        hsv = None\n        if isinstance(ss_mod, AutoIdentResult):\n            hsv = ss_mod.singular_values\n            # Fallback to model info if not in result\n            if hsv is None and isinstance(ss_mod.model, StateSpaceModel):\n                hsv = ss_mod.model.info.get(\"Hankel singular values\")\n        elif isinstance(ss_mod, StateSpaceModel):\n            hsv = ss_mod.info.get(\"Hankel singular values\")\n\n        if hsv is not None:\n            hsv_scaled = hsv / np.sum(hsv)\n            ax.bar(np.arange(1, len(hsv_scaled) + 1), hsv_scaled, color=col)\n            ax.set_title(\"Hankel Singular Values\")\n            ax.set_xlabel(\"State\")\n            ax.set_ylabel(\"Normalized HSV\")\n        else:\n            ax.text(0.5, 0.5, \"No HSV data available\", ha=\"center\", va=\"center\")\n\n    @staticmethod\n    def _time_series(fig: MplFigure, ax: Axes, data: SysIdData, col: str = \"#1f77b4\"):\n        t = data.time\n\n        for key, val in data.series.items():\n            ax.plot(t, val, label=key)  # Use default colors for multiple series\n\n        ax.set_title(\"Time series\")\n        ax.legend()\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True, alpha=0.3)\n\n    @staticmethod\n    def _compare(fig: MplFigure, ax: Axes, obj: Dict[str, Any], col: str = \"#1f77b4\"):\n        mods = obj.get(\"mod\", [])\n        if not isinstance(mods, list):\n            mods = [mods]\n\n        data = obj.get(\"data\")\n        y_name = obj.get(\"y_name\")\n        u_name = obj.get(\"u_name\")\n\n        if data is None or y_name is None or u_name is None:\n            return\n\n        t = data.time\n        ax.plot(t, data[y_name], \"--k\", label=\"Measured\", alpha=0.6)\n\n        for i, m in enumerate(mods):\n            # Cycle colors for models\n            c = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][(i) % 10]\n\n            if isinstance(m, LTIModel):\n                res = m.simulate(data[u_name], uncertainty=True)\n                if len(res) == 2:\n                    y_hat, y_std = res\n                else:\n                    y_hat = res\n                    y_std = None\n            else:\n                y_hat = m.simulate(data[u_name])\n                y_std = None\n\n            fit = m.compare(data[y_name], data[u_name])\n            ax.plot(t, y_hat, color=c, label=f\"Model {i + 1} (Fit={fit * 100:.1f}%)\")\n\n            if y_std is not None:\n                y_hat = y_hat.ravel()\n                y_std = y_std.ravel()\n                ax.fill_between(t, y_hat - 2 * y_std, y_hat + 2 * y_std, color=c, alpha=0.2)\n\n        ax.set_title(\"Model Comparison\")\n        ax.legend()\n        ax.set_xlabel(\"Time [s]\")\n        ax.set_ylabel(y_name)\n        ax.grid(True, alpha=0.3)\n\n    @staticmethod\n    def _residuals_acf(fig: MplFigure, ax: Axes, obj: Dict[str, Any], col: str = \"#1f77b4\"):\n        mod = obj.get(\"mod\")\n        data = obj.get(\"data\")\n\n        if mod is None or data is None:\n            return\n\n        # Handle list of models (take first one)\n        if isinstance(mod, list):\n            if len(mod) == 0:\n                return\n            mod = mod[0]\n\n        try:\n            res_analysis = compute_residuals_analysis(mod, data)\n        except ValueError:\n            return\n\n        acf = res_analysis[\"acf\"]\n        lags = res_analysis[\"lags\"]\n        conf_interval = res_analysis[\"conf_interval\"]\n\n        # Handle NaNs\n        if np.any(np.isnan(acf)):\n            acf = np.nan_to_num(acf)\n\n        mask = (lags &gt; -50) &amp; (lags &lt; 50)\n\n        ax.stem(lags[mask], acf[mask], markerfmt=\".\", label=\"ACF\")\n        ax.axhspan(-conf_interval, conf_interval, alpha=0.3, color=\"red\")\n        ax.set_title(\"Residuals ACF (Output 0)\")\n        ax.set_ylabel(\"ACF\")\n        ax.set_xlabel(\"Lag\")\n        ax.grid(True, alpha=0.3)\n\n    @staticmethod\n    def _residuals_ccf(fig: MplFigure, ax: Axes, obj: Dict[str, Any], col: str = \"#1f77b4\"):\n        mod = obj.get(\"mod\")\n        data = obj.get(\"data\")\n\n        if mod is None or data is None:\n            return\n\n        # Handle list of models (take first one)\n        if isinstance(mod, list):\n            if len(mod) == 0:\n                return\n            mod = mod[0]\n\n        try:\n            res_analysis = compute_residuals_analysis(mod, data)\n        except ValueError:\n            return\n\n        ccf = res_analysis[\"ccf\"]\n        lags = res_analysis[\"lags\"]\n        conf_interval = res_analysis[\"conf_interval\"]\n\n        # Handle NaNs\n        if np.any(np.isnan(ccf)):\n            ccf = np.nan_to_num(ccf)\n\n        mask = (lags &gt; -50) &amp; (lags &lt; 50)\n\n        ax.stem(lags[mask], ccf[mask], markerfmt=\".\", linefmt=\"C1-\", label=\"CCF\")\n        ax.axhspan(-conf_interval, conf_interval, alpha=0.3, color=\"red\")\n        ax.set_title(\"Residuals CCF (Output 0 vs Input 0)\")\n        ax.set_ylabel(\"CCF\")\n        ax.set_xlabel(\"Lag\")\n        ax.grid(True, alpha=0.3)\n</code></pre>"},{"location":"api/figure/#llsi.figure.Figure.__init__","title":"<code>__init__(figsize=(16, 9))</code>","text":"<p>Initialize the Figure context manager.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for the figure.</p> <code>(16, 9)</code> Source code in <code>src/llsi/figure.py</code> <pre><code>def __init__(self, figsize: Tuple[int, int] = (16, 9)):\n    \"\"\"\n    Initialize the Figure context manager.\n\n    Args:\n        figsize: Tuple of (width, height) for the figure.\n    \"\"\"\n    if plt is None:\n        raise ImportError(\"matplotlib is required for plotting. Install it with 'pip install llsi[plot]'.\")\n\n    self.objects: List[Any] = []\n    self.plot_types: List[Optional[str]] = []\n    self.place: List[int] = []\n\n    self.registry = {\n        \"impulse\": self._impulse,\n        \"step\": self._step,\n        \"frequency\": self._frequency,\n        \"hsv\": self._hsv,\n        \"time_series\": self._time_series,\n        \"compare\": self._compare,\n        \"residuals\": self._residuals_acf,\n        \"residuals_acf\": self._residuals_acf,\n        \"residuals_ccf\": self._residuals_ccf,\n    }\n\n    self.figsize = figsize\n    self.counter = 0\n    self.colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n    self.logger = logging.getLogger(__name__)\n    self.fig: Optional[MplFigure] = None\n    self.ax: Optional[Union[Axes, np.ndarray]] = None\n</code></pre>"},{"location":"api/figure/#llsi.figure.Figure.plot","title":"<code>plot(obj, plot_type=None)</code>","text":"<p>Add an object to be plotted.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Any, List[Any]]</code> <p>The object(s) to plot (LTIModel, SysIdData, etc.)</p> required <code>plot_type</code> <code>Optional[str]</code> <p>The type of plot ('impulse', 'step', 'frequency', 'hsv', 'time_series', 'compare').        If None, inferred from object type.</p> <code>None</code> Source code in <code>src/llsi/figure.py</code> <pre><code>def plot(self, obj: Union[Any, List[Any]], plot_type: Optional[str] = None):\n    \"\"\"\n    Add an object to be plotted.\n\n    Args:\n        obj: The object(s) to plot (LTIModel, SysIdData, etc.)\n        plot_type: The type of plot ('impulse', 'step', 'frequency', 'hsv', 'time_series', 'compare').\n                   If None, inferred from object type.\n    \"\"\"\n    if not isinstance(obj, (list, tuple)):\n        obj_list = [obj]\n    else:\n        obj_list = obj\n\n    if plot_type == \"residuals\":\n        # Special case: residuals implies both ACF and CCF\n        # We add two plots\n        for o in obj_list:\n            self.objects.append(o)\n            self.plot_types.append(\"residuals_acf\")\n            self.place.append(self.counter)\n            self.counter += 1\n\n            self.objects.append(o)\n            self.plot_types.append(\"residuals_ccf\")\n            self.place.append(self.counter)\n            self.counter += 1\n        return\n\n    for o in obj_list:\n        self.objects.append(o)\n        self.plot_types.append(plot_type)\n        self.place.append(self.counter)\n\n    self.counter += 1\n</code></pre>"},{"location":"api/ltimodel/","title":"LTI Model","text":""},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel","title":"<code>llsi.ltimodel.LTIModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for Linear Time-Invariant (LTI) models.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>class LTIModel(ABC):\n    \"\"\"\n    Abstract base class for Linear Time-Invariant (LTI) models.\n    \"\"\"\n\n    def __init__(\n        self,\n        Ts: float = 1.0,\n        nu: int = 1,\n        ny: int = 1,\n        input_names: Optional[List[str]] = None,\n        output_names: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize the LTI model.\n\n        Args:\n            Ts: Sampling time in seconds.\n            nu: Number of inputs.\n            ny: Number of outputs.\n            input_names: List of input channel names.\n            output_names: List of output channel names.\n        \"\"\"\n        if input_names is None:\n            input_names = []\n        if output_names is None:\n            output_names = []\n\n        self.Ts = Ts\n        self.info = {}\n        self.nu = nu\n        self.ny = ny\n        self.input_names = input_names\n        self.output_names = output_names\n\n        # Identification results\n        self.aic: Optional[float] = None\n        self.bic: Optional[float] = None\n        self.residuals: Optional[np.ndarray] = None\n        self.residuals_analysis: Optional[Dict[str, Any]] = None\n\n    def impulse_response(\n        self, N: int = 100, uncertainty: bool = False\n    ) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n        \"\"\"\n        Simulate the impulse response of the system.\n\n        Args:\n            N: Number of time steps to simulate.\n            uncertainty: If True, return standard deviation of the response.\n\n        Returns:\n            If uncertainty is False:\n                Tuple[np.ndarray, np.ndarray]: Time vector and output response.\n            If uncertainty is True:\n                Tuple[np.ndarray, np.ndarray, np.ndarray]: Time vector, output response, and standard deviation.\n        \"\"\"\n        t = np.linspace(0, (N - 1) * self.Ts, N)\n        u = np.zeros((N, self.nu))\n        # Impulse with area 1: height = 1/Ts, width = Ts\n        u[0, :] = 1.0 / self.Ts\n\n        y = self.simulate(u)\n\n        if uncertainty:\n            if not hasattr(self, \"cov\") or self.cov is None:\n                return t, y, None\n\n            def func():\n                return self.simulate(u).ravel()\n\n            y_std = self._propagate_uncertainty(func)\n            y_std = y_std.reshape(y.shape)\n            return t, y, y_std\n\n        return t, y\n\n    def step_response(\n        self, N: int = 100, uncertainty: bool = False\n    ) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n        \"\"\"\n        Simulate the step response of the system.\n\n        Args:\n            N: Number of time steps to simulate.\n            uncertainty: If True, return standard deviation of the response.\n\n        Returns:\n            If uncertainty is False:\n                Tuple[np.ndarray, np.ndarray]: Time vector and output response.\n            If uncertainty is True:\n                Tuple[np.ndarray, np.ndarray, np.ndarray]: Time vector, output response, and standard deviation.\n        \"\"\"\n        t = np.linspace(0, (N - 1) * self.Ts, N)\n        u = np.ones((N, self.nu))\n\n        y = self.simulate(u)\n\n        if uncertainty:\n            if not hasattr(self, \"cov\") or self.cov is None:\n                return t, y, None\n\n            def func():\n                return self.simulate(u).ravel()\n\n            y_std = self._propagate_uncertainty(func)\n            y_std = y_std.reshape(y.shape)\n            return t, y, y_std\n\n        return t, y\n\n    def _propagate_uncertainty(self, func) -&gt; np.ndarray:\n        \"\"\"\n        Compute standard deviation of func() output using error propagation.\n\n        Args:\n            func: Callable that returns a 1D array (or scalar).\n                  It should use the model's current parameters.\n\n        Returns:\n            std: Standard deviation of the output (same shape as func output).\n        \"\"\"\n        if not hasattr(self, \"vectorize\") or not hasattr(self, \"reshape\"):\n            raise NotImplementedError(\"Model must implement vectorize() and reshape() for uncertainty propagation.\")\n\n        theta_opt = self.vectorize()\n        n_params = len(theta_opt)\n        epsilon = 1e-8\n\n        # Compute nominal output\n        y_nominal = func()\n        n_out = y_nominal.size\n\n        # Compute Jacobian\n        # Handle complex output if necessary, though currently we wrap to real\n        is_complex = np.iscomplexobj(y_nominal)\n        dtype = np.complex128 if is_complex else np.float64\n\n        J = np.zeros((n_out, n_params), dtype=dtype)\n\n        for i in range(n_params):\n            theta_perturbed = theta_opt.copy()\n            theta_perturbed[i] += epsilon\n\n            self.reshape(theta_perturbed)\n            y_perturbed = func()\n\n            J[:, i] = (y_perturbed - y_nominal) / epsilon\n\n        # Restore parameters\n        self.reshape(theta_opt)\n\n        # Compute variance: diag(J @ cov @ J.T)\n        cov = self.cov\n\n        if is_complex:\n            var = np.sum((J @ cov) * np.conj(J), axis=1)\n            var = np.real(var)\n        else:\n            var = np.sum((J @ cov) * J, axis=1)\n\n        var = np.maximum(var, 0.0)\n\n        return np.sqrt(var)\n\n    def compare(self, y: np.ndarray, u: np.ndarray) -&gt; float:\n        \"\"\"\n        Compare model output with measured output.\n\n        Args:\n            y: Measured output.\n            u: Input signal.\n\n        Returns:\n            float: Normalized Root Mean Squared Error (NRMSE) fit index (1 - NRMSE).\n                   1.0 means perfect fit.\n        \"\"\"\n        y_hat = self.simulate(u)\n        # NRMSE returns error ratio, so 1 - NRMSE is the fit\n        return 1.0 - self.NRMSE(y, y_hat)\n\n    @staticmethod\n    def residuals(y: np.ndarray, y_hat: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Calculate residuals (error).\"\"\"\n        return y.ravel() - y_hat.ravel()\n\n    @staticmethod\n    def SE(e: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Squared Error.\"\"\"\n        return np.power(e.ravel(), 2)\n\n    @staticmethod\n    def SSE(e: np.ndarray) -&gt; float:\n        \"\"\"Sum of Squared Errors.\"\"\"\n        e_ = e.ravel()\n        return float(e_.T @ e_)\n\n    @staticmethod\n    def MSE(e: np.ndarray) -&gt; float:\n        \"\"\"Mean Squared Error.\"\"\"\n        e_ = e.ravel()\n        return float((1.0 / len(e_)) * (e_.T @ e_))\n\n    @staticmethod\n    def RMSE(e: np.ndarray) -&gt; float:\n        \"\"\"Root Mean Squared Error.\"\"\"\n        return np.sqrt(LTIModel.MSE(e))\n\n    @staticmethod\n    def NRMSE(y: np.ndarray, y_hat: np.ndarray, normalization: str = \"matlab\") -&gt; float:\n        \"\"\"\n        Normalized Root Mean Squared Error.\n\n        Args:\n            y: True output.\n            y_hat: Predicted output.\n            normalization: Normalization method ('matlab', 'mean', 'ptp').\n                           'matlab': norm(e) / norm(y - mean(y))\n                           'mean': RMSE(e) / mean(y)\n                           'ptp': RMSE(e) / peak_to_peak(y)\n\n        Returns:\n            float: NRMSE value.\n        \"\"\"\n        e = LTIModel.residuals(y, y_hat)\n        if normalization == \"matlab\":\n            # MATLAB's 'goodnessOfFit' NRMSE cost function:\n            # norm(y - y_hat) / norm(y - mean(y))\n            nrmse = np.linalg.norm(e) / np.linalg.norm(y - np.mean(y))\n        elif normalization == \"mean\":\n            nrmse = LTIModel.RMSE(e) / np.mean(y)\n        elif normalization == \"ptp\":\n            nrmse = LTIModel.RMSE(e) / np.ptp(y)\n        else:\n            raise ValueError(f\"Unknown normalization method {normalization}\")\n        return float(nrmse)\n\n    @abstractmethod\n    def simulate(self, u: np.ndarray, uncertainty: bool = False) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"\n        Simulate the model response to input u.\n\n        Args:\n            u: Input signal array of shape (N, nu).\n            uncertainty: If True, return standard deviation of the response.\n\n        Returns:\n            If uncertainty is False:\n                np.ndarray: Output signal array of shape (N, ny).\n            If uncertainty is True:\n                Tuple[np.ndarray, np.ndarray]: Output signal and standard deviation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def frequency_response(\n        self, omega: Optional[np.ndarray] = None, uncertainty: bool = False\n    ) -&gt; Union[\n        Tuple[np.ndarray, np.ndarray],\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n    ]:\n        \"\"\"\n        Calculate frequency response.\n\n        Args:\n            omega: Frequency vector (rad/s). If None, a default range is used.\n            uncertainty: If True, return standard deviation of magnitude and phase.\n\n        Returns:\n            If uncertainty is False:\n                Tuple[np.ndarray, np.ndarray]: Frequency vector and complex response.\n            If uncertainty is True:\n                Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n                    Frequency vector, complex response, magnitude std, phase std.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.MSE","title":"<code>MSE(e)</code>  <code>staticmethod</code>","text":"<p>Mean Squared Error.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@staticmethod\ndef MSE(e: np.ndarray) -&gt; float:\n    \"\"\"Mean Squared Error.\"\"\"\n    e_ = e.ravel()\n    return float((1.0 / len(e_)) * (e_.T @ e_))\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.NRMSE","title":"<code>NRMSE(y, y_hat, normalization='matlab')</code>  <code>staticmethod</code>","text":"<p>Normalized Root Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>True output.</p> required <code>y_hat</code> <code>ndarray</code> <p>Predicted output.</p> required <code>normalization</code> <code>str</code> <p>Normalization method ('matlab', 'mean', 'ptp').            'matlab': norm(e) / norm(y - mean(y))            'mean': RMSE(e) / mean(y)            'ptp': RMSE(e) / peak_to_peak(y)</p> <code>'matlab'</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>NRMSE value.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@staticmethod\ndef NRMSE(y: np.ndarray, y_hat: np.ndarray, normalization: str = \"matlab\") -&gt; float:\n    \"\"\"\n    Normalized Root Mean Squared Error.\n\n    Args:\n        y: True output.\n        y_hat: Predicted output.\n        normalization: Normalization method ('matlab', 'mean', 'ptp').\n                       'matlab': norm(e) / norm(y - mean(y))\n                       'mean': RMSE(e) / mean(y)\n                       'ptp': RMSE(e) / peak_to_peak(y)\n\n    Returns:\n        float: NRMSE value.\n    \"\"\"\n    e = LTIModel.residuals(y, y_hat)\n    if normalization == \"matlab\":\n        # MATLAB's 'goodnessOfFit' NRMSE cost function:\n        # norm(y - y_hat) / norm(y - mean(y))\n        nrmse = np.linalg.norm(e) / np.linalg.norm(y - np.mean(y))\n    elif normalization == \"mean\":\n        nrmse = LTIModel.RMSE(e) / np.mean(y)\n    elif normalization == \"ptp\":\n        nrmse = LTIModel.RMSE(e) / np.ptp(y)\n    else:\n        raise ValueError(f\"Unknown normalization method {normalization}\")\n    return float(nrmse)\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.RMSE","title":"<code>RMSE(e)</code>  <code>staticmethod</code>","text":"<p>Root Mean Squared Error.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@staticmethod\ndef RMSE(e: np.ndarray) -&gt; float:\n    \"\"\"Root Mean Squared Error.\"\"\"\n    return np.sqrt(LTIModel.MSE(e))\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.SE","title":"<code>SE(e)</code>  <code>staticmethod</code>","text":"<p>Squared Error.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@staticmethod\ndef SE(e: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Squared Error.\"\"\"\n    return np.power(e.ravel(), 2)\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.SSE","title":"<code>SSE(e)</code>  <code>staticmethod</code>","text":"<p>Sum of Squared Errors.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@staticmethod\ndef SSE(e: np.ndarray) -&gt; float:\n    \"\"\"Sum of Squared Errors.\"\"\"\n    e_ = e.ravel()\n    return float(e_.T @ e_)\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.__init__","title":"<code>__init__(Ts=1.0, nu=1, ny=1, input_names=None, output_names=None)</code>","text":"<p>Initialize the LTI model.</p> <p>Parameters:</p> Name Type Description Default <code>Ts</code> <code>float</code> <p>Sampling time in seconds.</p> <code>1.0</code> <code>nu</code> <code>int</code> <p>Number of inputs.</p> <code>1</code> <code>ny</code> <code>int</code> <p>Number of outputs.</p> <code>1</code> <code>input_names</code> <code>Optional[List[str]]</code> <p>List of input channel names.</p> <code>None</code> <code>output_names</code> <code>Optional[List[str]]</code> <p>List of output channel names.</p> <code>None</code> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>def __init__(\n    self,\n    Ts: float = 1.0,\n    nu: int = 1,\n    ny: int = 1,\n    input_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n):\n    \"\"\"\n    Initialize the LTI model.\n\n    Args:\n        Ts: Sampling time in seconds.\n        nu: Number of inputs.\n        ny: Number of outputs.\n        input_names: List of input channel names.\n        output_names: List of output channel names.\n    \"\"\"\n    if input_names is None:\n        input_names = []\n    if output_names is None:\n        output_names = []\n\n    self.Ts = Ts\n    self.info = {}\n    self.nu = nu\n    self.ny = ny\n    self.input_names = input_names\n    self.output_names = output_names\n\n    # Identification results\n    self.aic: Optional[float] = None\n    self.bic: Optional[float] = None\n    self.residuals: Optional[np.ndarray] = None\n    self.residuals_analysis: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.compare","title":"<code>compare(y, u)</code>","text":"<p>Compare model output with measured output.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>Measured output.</p> required <code>u</code> <code>ndarray</code> <p>Input signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Normalized Root Mean Squared Error (NRMSE) fit index (1 - NRMSE).    1.0 means perfect fit.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>def compare(self, y: np.ndarray, u: np.ndarray) -&gt; float:\n    \"\"\"\n    Compare model output with measured output.\n\n    Args:\n        y: Measured output.\n        u: Input signal.\n\n    Returns:\n        float: Normalized Root Mean Squared Error (NRMSE) fit index (1 - NRMSE).\n               1.0 means perfect fit.\n    \"\"\"\n    y_hat = self.simulate(u)\n    # NRMSE returns error ratio, so 1 - NRMSE is the fit\n    return 1.0 - self.NRMSE(y, y_hat)\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.frequency_response","title":"<code>frequency_response(omega=None, uncertainty=False)</code>  <code>abstractmethod</code>","text":"<p>Calculate frequency response.</p> <p>Parameters:</p> Name Type Description Default <code>omega</code> <code>Optional[ndarray]</code> <p>Frequency vector (rad/s). If None, a default range is used.</p> <code>None</code> <code>uncertainty</code> <code>bool</code> <p>If True, return standard deviation of magnitude and phase.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray]]</code> <p>If uncertainty is False: Tuple[np.ndarray, np.ndarray]: Frequency vector and complex response.</p> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray]]</code> <p>If uncertainty is True: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:     Frequency vector, complex response, magnitude std, phase std.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@abstractmethod\ndef frequency_response(\n    self, omega: Optional[np.ndarray] = None, uncertainty: bool = False\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray],\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n]:\n    \"\"\"\n    Calculate frequency response.\n\n    Args:\n        omega: Frequency vector (rad/s). If None, a default range is used.\n        uncertainty: If True, return standard deviation of magnitude and phase.\n\n    Returns:\n        If uncertainty is False:\n            Tuple[np.ndarray, np.ndarray]: Frequency vector and complex response.\n        If uncertainty is True:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n                Frequency vector, complex response, magnitude std, phase std.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.impulse_response","title":"<code>impulse_response(N=100, uncertainty=False)</code>","text":"<p>Simulate the impulse response of the system.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>Number of time steps to simulate.</p> <code>100</code> <code>uncertainty</code> <code>bool</code> <p>If True, return standard deviation of the response.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray]]</code> <p>If uncertainty is False: Tuple[np.ndarray, np.ndarray]: Time vector and output response.</p> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray]]</code> <p>If uncertainty is True: Tuple[np.ndarray, np.ndarray, np.ndarray]: Time vector, output response, and standard deviation.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>def impulse_response(\n    self, N: int = 100, uncertainty: bool = False\n) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Simulate the impulse response of the system.\n\n    Args:\n        N: Number of time steps to simulate.\n        uncertainty: If True, return standard deviation of the response.\n\n    Returns:\n        If uncertainty is False:\n            Tuple[np.ndarray, np.ndarray]: Time vector and output response.\n        If uncertainty is True:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: Time vector, output response, and standard deviation.\n    \"\"\"\n    t = np.linspace(0, (N - 1) * self.Ts, N)\n    u = np.zeros((N, self.nu))\n    # Impulse with area 1: height = 1/Ts, width = Ts\n    u[0, :] = 1.0 / self.Ts\n\n    y = self.simulate(u)\n\n    if uncertainty:\n        if not hasattr(self, \"cov\") or self.cov is None:\n            return t, y, None\n\n        def func():\n            return self.simulate(u).ravel()\n\n        y_std = self._propagate_uncertainty(func)\n        y_std = y_std.reshape(y.shape)\n        return t, y, y_std\n\n    return t, y\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.residuals","title":"<code>residuals(y, y_hat)</code>  <code>staticmethod</code>","text":"<p>Calculate residuals (error).</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@staticmethod\ndef residuals(y: np.ndarray, y_hat: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate residuals (error).\"\"\"\n    return y.ravel() - y_hat.ravel()\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.simulate","title":"<code>simulate(u, uncertainty=False)</code>  <code>abstractmethod</code>","text":"<p>Simulate the model response to input u.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>ndarray</code> <p>Input signal array of shape (N, nu).</p> required <code>uncertainty</code> <code>bool</code> <p>If True, return standard deviation of the response.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>If uncertainty is False: np.ndarray: Output signal array of shape (N, ny).</p> <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>If uncertainty is True: Tuple[np.ndarray, np.ndarray]: Output signal and standard deviation.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>@abstractmethod\ndef simulate(self, u: np.ndarray, uncertainty: bool = False) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Simulate the model response to input u.\n\n    Args:\n        u: Input signal array of shape (N, nu).\n        uncertainty: If True, return standard deviation of the response.\n\n    Returns:\n        If uncertainty is False:\n            np.ndarray: Output signal array of shape (N, ny).\n        If uncertainty is True:\n            Tuple[np.ndarray, np.ndarray]: Output signal and standard deviation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ltimodel/#llsi.ltimodel.LTIModel.step_response","title":"<code>step_response(N=100, uncertainty=False)</code>","text":"<p>Simulate the step response of the system.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>Number of time steps to simulate.</p> <code>100</code> <code>uncertainty</code> <code>bool</code> <p>If True, return standard deviation of the response.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray]]</code> <p>If uncertainty is False: Tuple[np.ndarray, np.ndarray]: Time vector and output response.</p> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray]]</code> <p>If uncertainty is True: Tuple[np.ndarray, np.ndarray, np.ndarray]: Time vector, output response, and standard deviation.</p> Source code in <code>src/llsi/ltimodel.py</code> <pre><code>def step_response(\n    self, N: int = 100, uncertainty: bool = False\n) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Simulate the step response of the system.\n\n    Args:\n        N: Number of time steps to simulate.\n        uncertainty: If True, return standard deviation of the response.\n\n    Returns:\n        If uncertainty is False:\n            Tuple[np.ndarray, np.ndarray]: Time vector and output response.\n        If uncertainty is True:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: Time vector, output response, and standard deviation.\n    \"\"\"\n    t = np.linspace(0, (N - 1) * self.Ts, N)\n    u = np.ones((N, self.nu))\n\n    y = self.simulate(u)\n\n    if uncertainty:\n        if not hasattr(self, \"cov\") or self.cov is None:\n            return t, y, None\n\n        def func():\n            return self.simulate(u).ravel()\n\n        y_std = self._propagate_uncertainty(func)\n        y_std = y_std.reshape(y.shape)\n        return t, y, y_std\n\n    return t, y\n</code></pre>"},{"location":"api/polynomialmodel/","title":"Polynomial Model (ARX / OE)","text":""},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel","title":"<code>llsi.polynomialmodel.PolynomialModel</code>","text":"<p>               Bases: <code>LTIModel</code></p> <p>Polynomial model representation (SISO).</p> <p>Represents a system of the form: A(q) y(t) = B(q) u(t-nk) + e(t)</p> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>class PolynomialModel(LTIModel):\n    \"\"\"\n    Polynomial model representation (SISO).\n\n    Represents a system of the form:\n    A(q) y(t) = B(q) u(t-nk) + e(t)\n    \"\"\"\n\n    def __init__(\n        self,\n        a: Optional[Union[np.ndarray, List[float]]] = None,\n        b: Optional[Union[np.ndarray, List[float]]] = None,\n        na: int = 1,\n        nb: int = 1,\n        nu: int = 1,\n        ny: int = 1,\n        nk: int = 0,\n        cov: Optional[np.ndarray] = None,\n        Ts: float = 1.0,\n        input_names: Optional[List[str]] = None,\n        output_names: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize the polynomial model.\n\n        Args:\n            a: Denominator coefficients A(q).\n            b: Numerator coefficients B(q).\n            na: Order of A(q).\n            nb: Order of B(q).\n            nu: Number of inputs (must be 1).\n            ny: Number of outputs (must be 1).\n            nk: Input delay (samples).\n            cov: Covariance matrix of parameters.\n            Ts: Sampling time.\n            input_names: List of input names.\n            output_names: List of output names.\n        \"\"\"\n        if input_names is None:\n            input_names = []\n        if output_names is None:\n            output_names = []\n        super().__init__(Ts=Ts, input_names=input_names, output_names=output_names)\n\n        if a is not None:\n            self.a = np.atleast_1d(a).astype(float)\n            self.na = len(self.a)\n            self.ny = 1  # Enforce SISO for now based on original code logic\n        else:\n            self.na = na\n            self.ny = ny\n            self.a = np.ones(self.na)\n\n        if b is not None:\n            self.b = np.atleast_1d(b).astype(float)\n            self.nb = len(self.b)\n            self.nu = 1  # Enforce SISO\n        else:\n            self.nb = nb\n            self.nu = nu\n            self.b = np.ones(self.nb)\n\n        # Original code raised error for MIMO, keeping that constraint\n        if self.ny &gt; 1 or (a is not None and np.ndim(a) &gt; 1 and np.shape(a)[1] &gt; 1):\n            # Check if it was initialized with 2D array implying MIMO\n            # The original code did: self.a = np.atleast_2d(a).T; self.ny = self.a.shape[1]\n            # If user passed 1D list, atleast_2d makes it (1, N), .T makes it (N, 1), so ny=1.\n            # If user passed 2D (N, M), .T makes it (M, N)? No, (N, M).T is (M, N).\n            # Let's stick to SISO as explicitly stated in original code.\n            pass\n\n        if self.ny &gt; 1:\n            raise ValueError(\"System seems to have multiple outputs. This is not implemented.\")\n\n        if self.nu &gt; 1:\n            raise ValueError(\"System seems to have multiple inputs. This is not implemented.\")\n\n        # Normalize\n        if len(self.a) &gt; 0 and self.a[0] != 0:\n            norm_factor = self.a[0]\n            self.b = self.b / norm_factor\n            self.a = self.a / norm_factor\n\n        self.nk = nk\n        self.cov = cov\n\n    def simulate(\n        self, u: Union[np.ndarray, List[float]], uncertainty: bool = False\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"\n        Simulate the model response.\n\n        Args:\n            u: Input signal.\n            uncertainty: If True, return standard deviation of the response.\n\n        Returns:\n            If uncertainty is False:\n                np.ndarray: Simulated output.\n            If uncertainty is True:\n                Tuple[np.ndarray, np.ndarray]: Simulated output and standard deviation.\n        \"\"\"\n        u_arr = np.atleast_1d(u).ravel()\n\n        # Handle delay by padding input or coefficients\n        # lfilter: a[0]*y[n] = b[0]*x[n] + ...\n        # We have y[n] = -a[1]*y[n-1]... + b[0]*u[n-nk]...\n        # So we can pad b with nk zeros at the front.\n\n        if self.nk &gt; 0:\n            b_padded = np.concatenate((np.zeros(self.nk), self.b))\n        else:\n            b_padded = self.b\n\n        # scipy.signal.lfilter is much faster than python loops\n        y = scipy.signal.lfilter(b_padded, self.a, u_arr)\n        y = y.reshape(-1, 1)  # Return as (N, 1) to match LTIModel convention\n\n        if uncertainty:\n            if not hasattr(self, \"cov\") or self.cov is None:\n                return y, None\n\n            # Ensure u is correct shape for closure\n            u_for_closure = np.array(u)\n\n            def func():\n                # We need to call simulate with uncertainty=False to avoid recursion\n                # But since we are inside simulate, we can just call the logic directly or call self.simulate(..., uncertainty=False)\n                return self.simulate(u_for_closure, uncertainty=False).ravel()\n\n            y_std = self._propagate_uncertainty(func)\n            y_std = y_std.reshape(y.shape)\n            return y, y_std\n\n        return y\n\n    def frequency_response(\n        self, omega: np.ndarray = np.logspace(-3, 2), uncertainty: bool = False\n    ) -&gt; Union[\n        Tuple[np.ndarray, np.ndarray],\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n    ]:\n        \"\"\"\n        Compute frequency response.\n\n        Args:\n            omega: Frequencies (rad/s).\n            uncertainty: If True, return standard deviation of magnitude and phase.\n\n        Returns:\n            If uncertainty is False:\n                Tuple[np.ndarray, np.ndarray]: (omega, H)\n            If uncertainty is True:\n                Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: (omega, H, mag_std, phase_std)\n        \"\"\"\n        # H(z) = (B(z) / A(z)) * z^(-nk)\n        # z = exp(j * omega * Ts)\n\n        # w, h = scipy.signal.freqz(self.b, self.a, worN=omega * self.Ts, fs=2 * np.pi)\n\n        # freqz returns response for B(z)/A(z). We need to add delay term z^(-nk)\n        # But wait, freqz expects w in [0, pi) usually, or if fs is given, in Hz?\n        # \"If fs is not specified, worN is assumed to be in the same units as fs (radians/sample).\"\n        # Actually, let's stick to manual calculation or use freqz correctly.\n        # Manual calculation is safer for arbitrary omega units if we are careful.\n\n        z = np.exp(1j * omega * self.Ts)\n\n        # Evaluate polynomials\n        # polyval expects coefficients from highest degree to lowest?\n        # No, standard polynomial is p[0]*x^(N-1) + ...\n        # But here we have B(z^-1) = b[0] + b[1]z^-1 + ...\n        # So we evaluate \\sum b[k] z^{-k}\n\n        # Using broadcasting\n        k_a = np.arange(len(self.a))\n        k_b = np.arange(len(self.b))\n\n        # (N_freq, N_coeff)\n        z_pow_a = np.power(z[:, None], -k_a[None, :])\n        z_pow_b = np.power(z[:, None], -k_b[None, :])\n\n        A_z = np.sum(self.a[None, :] * z_pow_a, axis=1)\n        B_z = np.sum(self.b[None, :] * z_pow_b, axis=1)\n\n        H = (B_z / A_z) * np.power(z, -self.nk)\n\n        if uncertainty:\n            if not hasattr(self, \"cov\") or self.cov is None:\n                return omega, H, None, None\n\n            def func():\n                # Call self.frequency_response with uncertainty=False\n                _, H_ = self.frequency_response(omega, uncertainty=False)\n                H_ = H_.ravel()\n                # Use unwrap to avoid discontinuities in phase gradient\n                return np.concatenate([np.abs(H_), np.unwrap(np.angle(H_))])\n\n            std = self._propagate_uncertainty(func)\n            n = H.size\n            mag_std = std[:n].reshape(H.shape)\n            phase_std = std[n:].reshape(H.shape)\n\n            return omega, H, mag_std, phase_std\n\n        return omega, H\n\n    def vectorize(self) -&gt; np.ndarray:\n        \"\"\"Return model parameters as a vector.\"\"\"\n        # Usually [b0, b1, ..., a1, a2, ...] (a0 is fixed to 1)\n        return np.hstack((self.b, self.a[1:])).ravel()\n\n    def reshape(self, theta: np.ndarray) -&gt; None:\n        \"\"\"Update model parameters from vector.\"\"\"\n        theta = np.array(theta).ravel()\n        n_b = len(self.b)\n        self.b = theta[:n_b]\n        self.a = np.hstack(([1.0], theta[n_b:]))\n\n    def to_tf(self, continuous: bool = False, method: str = \"bilinear\") -&gt; scipy.signal.TransferFunction:\n        \"\"\"\n        Convert the model to a Scipy TransferFunction representation.\n\n        Args:\n            continuous: If True, convert to a continuous-time system.\n            method: The method to use for discrete-to-continuous conversion if `continuous=True`.\n                    Options: 'bilinear' (Tustin), 'euler' (Forward Euler), 'backward_diff', 'zoh'.\n                    Default is 'bilinear'.\n\n        Returns:\n            scipy.signal.TransferFunction: The transfer function representation.\n        \"\"\"\n        # Note: This ignores nk if not handled carefully, but TransferFunction\n        # in scipy is usually continuous time or discrete with fixed dt.\n        # If discrete, we can pad numerator.\n        if self.nk &gt; 0:\n            num = np.concatenate((np.zeros(self.nk), self.b))\n        else:\n            num = self.b\n\n        if not continuous:\n            return scipy.signal.TransferFunction(num, self.a, dt=self.Ts)\n\n        # Convert to SS first\n        A, B, C, D = scipy.signal.tf2ss(num, self.a)\n\n        from .statespacemodel import StateSpaceModel\n\n        # Use StateSpaceModel's d2c logic\n        Ac, Bc, Cc, Dc = StateSpaceModel._d2c(A, B, C, D, self.Ts, method=method)\n\n        return scipy.signal.StateSpace(Ac, Bc, Cc, Dc).to_tf()\n\n    @classmethod\n    def from_scipy(cls, mod: Any) -&gt; \"PolynomialModel\":\n        \"\"\"Create from scipy system.\"\"\"\n        # Assuming mod has .dt, .num, .den or similar\n        if hasattr(mod, \"dt\"):\n            dt = mod.dt\n        else:\n            dt = 1.0\n\n        if isinstance(mod, scipy.signal.TransferFunction):\n            return cls(a=mod.den, b=mod.num, Ts=dt)\n\n        # Try converting\n        tf = mod.to_tf()\n        return cls(a=tf.den, b=tf.num, Ts=dt)\n\n    def __repr__(self) -&gt; str:\n        s = f\"PolynomialModel with Ts={self.Ts}\\n\"\n        s += f\"input(s): {self.input_names}\\n\"\n        s += f\"output(s): {self.output_names}\\n\"\n        s += f\"b: {self.b}\\n\"\n        s += f\"a: {self.a}\\n\"\n        s += f\"nk: {self.nk}\\n\"\n        return s\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n\n    def to_json(self, filename: Optional[str] = None) -&gt; str:\n        import json\n\n        data = {}\n        data[\"a\"] = self.a.tolist()\n        data[\"b\"] = self.b.tolist()\n        data[\"na\"] = self.na\n        data[\"nb\"] = self.nb\n        data[\"nk\"] = self.nk\n        data[\"Ts\"] = self.Ts\n        data[\"nu\"] = self.nu\n        data[\"ny\"] = self.ny\n        data[\"input_names\"] = self.input_names\n        data[\"output_names\"] = self.output_names\n\n        try:\n            data[\"info\"] = str(self.info)\n        except AttributeError:\n            data[\"info\"] = \"\"\n\n        if self.cov is not None:\n            data[\"cov\"] = self.cov.tolist()\n        else:\n            data[\"cov\"] = None\n\n        if filename is not None:\n            with open(filename, \"w\") as f:\n                json.dump(data, f, indent=4)\n            return \"\"\n\n        return json.dumps(data, indent=4)\n\n    @classmethod\n    def from_json(cls, filename: str) -&gt; \"PolynomialModel\":\n        import json\n\n        with open(filename) as f:\n            data = json.load(f)\n\n        cov = data.get(\"cov\")\n        if cov is not None:\n            cov = np.array(cov)\n\n        mod = PolynomialModel(\n            a=data[\"a\"],\n            b=data[\"b\"],\n            na=data.get(\"na\", 1),\n            nb=data.get(\"nb\", 1),\n            nk=data.get(\"nk\", 0),\n            Ts=data[\"Ts\"],\n            nu=data.get(\"nu\", 1),\n            ny=data.get(\"ny\", 1),\n            cov=cov,\n            input_names=data.get(\"input_names\"),\n            output_names=data.get(\"output_names\"),\n        )\n        if \"info\" in data:\n            mod.info = data[\"info\"]\n        return mod\n</code></pre>"},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel.__init__","title":"<code>__init__(a=None, b=None, na=1, nb=1, nu=1, ny=1, nk=0, cov=None, Ts=1.0, input_names=None, output_names=None)</code>","text":"<p>Initialize the polynomial model.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Denominator coefficients A(q).</p> <code>None</code> <code>b</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Numerator coefficients B(q).</p> <code>None</code> <code>na</code> <code>int</code> <p>Order of A(q).</p> <code>1</code> <code>nb</code> <code>int</code> <p>Order of B(q).</p> <code>1</code> <code>nu</code> <code>int</code> <p>Number of inputs (must be 1).</p> <code>1</code> <code>ny</code> <code>int</code> <p>Number of outputs (must be 1).</p> <code>1</code> <code>nk</code> <code>int</code> <p>Input delay (samples).</p> <code>0</code> <code>cov</code> <code>Optional[ndarray]</code> <p>Covariance matrix of parameters.</p> <code>None</code> <code>Ts</code> <code>float</code> <p>Sampling time.</p> <code>1.0</code> <code>input_names</code> <code>Optional[List[str]]</code> <p>List of input names.</p> <code>None</code> <code>output_names</code> <code>Optional[List[str]]</code> <p>List of output names.</p> <code>None</code> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>def __init__(\n    self,\n    a: Optional[Union[np.ndarray, List[float]]] = None,\n    b: Optional[Union[np.ndarray, List[float]]] = None,\n    na: int = 1,\n    nb: int = 1,\n    nu: int = 1,\n    ny: int = 1,\n    nk: int = 0,\n    cov: Optional[np.ndarray] = None,\n    Ts: float = 1.0,\n    input_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n):\n    \"\"\"\n    Initialize the polynomial model.\n\n    Args:\n        a: Denominator coefficients A(q).\n        b: Numerator coefficients B(q).\n        na: Order of A(q).\n        nb: Order of B(q).\n        nu: Number of inputs (must be 1).\n        ny: Number of outputs (must be 1).\n        nk: Input delay (samples).\n        cov: Covariance matrix of parameters.\n        Ts: Sampling time.\n        input_names: List of input names.\n        output_names: List of output names.\n    \"\"\"\n    if input_names is None:\n        input_names = []\n    if output_names is None:\n        output_names = []\n    super().__init__(Ts=Ts, input_names=input_names, output_names=output_names)\n\n    if a is not None:\n        self.a = np.atleast_1d(a).astype(float)\n        self.na = len(self.a)\n        self.ny = 1  # Enforce SISO for now based on original code logic\n    else:\n        self.na = na\n        self.ny = ny\n        self.a = np.ones(self.na)\n\n    if b is not None:\n        self.b = np.atleast_1d(b).astype(float)\n        self.nb = len(self.b)\n        self.nu = 1  # Enforce SISO\n    else:\n        self.nb = nb\n        self.nu = nu\n        self.b = np.ones(self.nb)\n\n    # Original code raised error for MIMO, keeping that constraint\n    if self.ny &gt; 1 or (a is not None and np.ndim(a) &gt; 1 and np.shape(a)[1] &gt; 1):\n        # Check if it was initialized with 2D array implying MIMO\n        # The original code did: self.a = np.atleast_2d(a).T; self.ny = self.a.shape[1]\n        # If user passed 1D list, atleast_2d makes it (1, N), .T makes it (N, 1), so ny=1.\n        # If user passed 2D (N, M), .T makes it (M, N)? No, (N, M).T is (M, N).\n        # Let's stick to SISO as explicitly stated in original code.\n        pass\n\n    if self.ny &gt; 1:\n        raise ValueError(\"System seems to have multiple outputs. This is not implemented.\")\n\n    if self.nu &gt; 1:\n        raise ValueError(\"System seems to have multiple inputs. This is not implemented.\")\n\n    # Normalize\n    if len(self.a) &gt; 0 and self.a[0] != 0:\n        norm_factor = self.a[0]\n        self.b = self.b / norm_factor\n        self.a = self.a / norm_factor\n\n    self.nk = nk\n    self.cov = cov\n</code></pre>"},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel.frequency_response","title":"<code>frequency_response(omega=np.logspace(-3, 2), uncertainty=False)</code>","text":"<p>Compute frequency response.</p> <p>Parameters:</p> Name Type Description Default <code>omega</code> <code>ndarray</code> <p>Frequencies (rad/s).</p> <code>logspace(-3, 2)</code> <code>uncertainty</code> <code>bool</code> <p>If True, return standard deviation of magnitude and phase.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray]]</code> <p>If uncertainty is False: Tuple[np.ndarray, np.ndarray]: (omega, H)</p> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray]]</code> <p>If uncertainty is True: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: (omega, H, mag_std, phase_std)</p> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>def frequency_response(\n    self, omega: np.ndarray = np.logspace(-3, 2), uncertainty: bool = False\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray],\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n]:\n    \"\"\"\n    Compute frequency response.\n\n    Args:\n        omega: Frequencies (rad/s).\n        uncertainty: If True, return standard deviation of magnitude and phase.\n\n    Returns:\n        If uncertainty is False:\n            Tuple[np.ndarray, np.ndarray]: (omega, H)\n        If uncertainty is True:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: (omega, H, mag_std, phase_std)\n    \"\"\"\n    # H(z) = (B(z) / A(z)) * z^(-nk)\n    # z = exp(j * omega * Ts)\n\n    # w, h = scipy.signal.freqz(self.b, self.a, worN=omega * self.Ts, fs=2 * np.pi)\n\n    # freqz returns response for B(z)/A(z). We need to add delay term z^(-nk)\n    # But wait, freqz expects w in [0, pi) usually, or if fs is given, in Hz?\n    # \"If fs is not specified, worN is assumed to be in the same units as fs (radians/sample).\"\n    # Actually, let's stick to manual calculation or use freqz correctly.\n    # Manual calculation is safer for arbitrary omega units if we are careful.\n\n    z = np.exp(1j * omega * self.Ts)\n\n    # Evaluate polynomials\n    # polyval expects coefficients from highest degree to lowest?\n    # No, standard polynomial is p[0]*x^(N-1) + ...\n    # But here we have B(z^-1) = b[0] + b[1]z^-1 + ...\n    # So we evaluate \\sum b[k] z^{-k}\n\n    # Using broadcasting\n    k_a = np.arange(len(self.a))\n    k_b = np.arange(len(self.b))\n\n    # (N_freq, N_coeff)\n    z_pow_a = np.power(z[:, None], -k_a[None, :])\n    z_pow_b = np.power(z[:, None], -k_b[None, :])\n\n    A_z = np.sum(self.a[None, :] * z_pow_a, axis=1)\n    B_z = np.sum(self.b[None, :] * z_pow_b, axis=1)\n\n    H = (B_z / A_z) * np.power(z, -self.nk)\n\n    if uncertainty:\n        if not hasattr(self, \"cov\") or self.cov is None:\n            return omega, H, None, None\n\n        def func():\n            # Call self.frequency_response with uncertainty=False\n            _, H_ = self.frequency_response(omega, uncertainty=False)\n            H_ = H_.ravel()\n            # Use unwrap to avoid discontinuities in phase gradient\n            return np.concatenate([np.abs(H_), np.unwrap(np.angle(H_))])\n\n        std = self._propagate_uncertainty(func)\n        n = H.size\n        mag_std = std[:n].reshape(H.shape)\n        phase_std = std[n:].reshape(H.shape)\n\n        return omega, H, mag_std, phase_std\n\n    return omega, H\n</code></pre>"},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel.from_scipy","title":"<code>from_scipy(mod)</code>  <code>classmethod</code>","text":"<p>Create from scipy system.</p> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>@classmethod\ndef from_scipy(cls, mod: Any) -&gt; \"PolynomialModel\":\n    \"\"\"Create from scipy system.\"\"\"\n    # Assuming mod has .dt, .num, .den or similar\n    if hasattr(mod, \"dt\"):\n        dt = mod.dt\n    else:\n        dt = 1.0\n\n    if isinstance(mod, scipy.signal.TransferFunction):\n        return cls(a=mod.den, b=mod.num, Ts=dt)\n\n    # Try converting\n    tf = mod.to_tf()\n    return cls(a=tf.den, b=tf.num, Ts=dt)\n</code></pre>"},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel.reshape","title":"<code>reshape(theta)</code>","text":"<p>Update model parameters from vector.</p> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>def reshape(self, theta: np.ndarray) -&gt; None:\n    \"\"\"Update model parameters from vector.\"\"\"\n    theta = np.array(theta).ravel()\n    n_b = len(self.b)\n    self.b = theta[:n_b]\n    self.a = np.hstack(([1.0], theta[n_b:]))\n</code></pre>"},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel.simulate","title":"<code>simulate(u, uncertainty=False)</code>","text":"<p>Simulate the model response.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Union[ndarray, List[float]]</code> <p>Input signal.</p> required <code>uncertainty</code> <code>bool</code> <p>If True, return standard deviation of the response.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>If uncertainty is False: np.ndarray: Simulated output.</p> <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>If uncertainty is True: Tuple[np.ndarray, np.ndarray]: Simulated output and standard deviation.</p> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>def simulate(\n    self, u: Union[np.ndarray, List[float]], uncertainty: bool = False\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Simulate the model response.\n\n    Args:\n        u: Input signal.\n        uncertainty: If True, return standard deviation of the response.\n\n    Returns:\n        If uncertainty is False:\n            np.ndarray: Simulated output.\n        If uncertainty is True:\n            Tuple[np.ndarray, np.ndarray]: Simulated output and standard deviation.\n    \"\"\"\n    u_arr = np.atleast_1d(u).ravel()\n\n    # Handle delay by padding input or coefficients\n    # lfilter: a[0]*y[n] = b[0]*x[n] + ...\n    # We have y[n] = -a[1]*y[n-1]... + b[0]*u[n-nk]...\n    # So we can pad b with nk zeros at the front.\n\n    if self.nk &gt; 0:\n        b_padded = np.concatenate((np.zeros(self.nk), self.b))\n    else:\n        b_padded = self.b\n\n    # scipy.signal.lfilter is much faster than python loops\n    y = scipy.signal.lfilter(b_padded, self.a, u_arr)\n    y = y.reshape(-1, 1)  # Return as (N, 1) to match LTIModel convention\n\n    if uncertainty:\n        if not hasattr(self, \"cov\") or self.cov is None:\n            return y, None\n\n        # Ensure u is correct shape for closure\n        u_for_closure = np.array(u)\n\n        def func():\n            # We need to call simulate with uncertainty=False to avoid recursion\n            # But since we are inside simulate, we can just call the logic directly or call self.simulate(..., uncertainty=False)\n            return self.simulate(u_for_closure, uncertainty=False).ravel()\n\n        y_std = self._propagate_uncertainty(func)\n        y_std = y_std.reshape(y.shape)\n        return y, y_std\n\n    return y\n</code></pre>"},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel.to_tf","title":"<code>to_tf(continuous=False, method='bilinear')</code>","text":"<p>Convert the model to a Scipy TransferFunction representation.</p> <p>Parameters:</p> Name Type Description Default <code>continuous</code> <code>bool</code> <p>If True, convert to a continuous-time system.</p> <code>False</code> <code>method</code> <code>str</code> <p>The method to use for discrete-to-continuous conversion if <code>continuous=True</code>.     Options: 'bilinear' (Tustin), 'euler' (Forward Euler), 'backward_diff', 'zoh'.     Default is 'bilinear'.</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>TransferFunction</code> <p>scipy.signal.TransferFunction: The transfer function representation.</p> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>def to_tf(self, continuous: bool = False, method: str = \"bilinear\") -&gt; scipy.signal.TransferFunction:\n    \"\"\"\n    Convert the model to a Scipy TransferFunction representation.\n\n    Args:\n        continuous: If True, convert to a continuous-time system.\n        method: The method to use for discrete-to-continuous conversion if `continuous=True`.\n                Options: 'bilinear' (Tustin), 'euler' (Forward Euler), 'backward_diff', 'zoh'.\n                Default is 'bilinear'.\n\n    Returns:\n        scipy.signal.TransferFunction: The transfer function representation.\n    \"\"\"\n    # Note: This ignores nk if not handled carefully, but TransferFunction\n    # in scipy is usually continuous time or discrete with fixed dt.\n    # If discrete, we can pad numerator.\n    if self.nk &gt; 0:\n        num = np.concatenate((np.zeros(self.nk), self.b))\n    else:\n        num = self.b\n\n    if not continuous:\n        return scipy.signal.TransferFunction(num, self.a, dt=self.Ts)\n\n    # Convert to SS first\n    A, B, C, D = scipy.signal.tf2ss(num, self.a)\n\n    from .statespacemodel import StateSpaceModel\n\n    # Use StateSpaceModel's d2c logic\n    Ac, Bc, Cc, Dc = StateSpaceModel._d2c(A, B, C, D, self.Ts, method=method)\n\n    return scipy.signal.StateSpace(Ac, Bc, Cc, Dc).to_tf()\n</code></pre>"},{"location":"api/polynomialmodel/#llsi.polynomialmodel.PolynomialModel.vectorize","title":"<code>vectorize()</code>","text":"<p>Return model parameters as a vector.</p> Source code in <code>src/llsi/polynomialmodel.py</code> <pre><code>def vectorize(self) -&gt; np.ndarray:\n    \"\"\"Return model parameters as a vector.\"\"\"\n    # Usually [b0, b1, ..., a1, a2, ...] (a0 is fixed to 1)\n    return np.hstack((self.b, self.a[1:])).ravel()\n</code></pre>"},{"location":"api/statespacemodel/","title":"State Space Model","text":""},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel","title":"<code>llsi.statespacemodel.StateSpaceModel</code>","text":"<p>               Bases: <code>LTIModel</code></p> <p>State Space model class.</p> <p>Represents a discrete-time system: x[k+1] = A x[k] + B u[k] y[k]   = C x[k] + D u[k]</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>class StateSpaceModel(LTIModel):\n    \"\"\"\n    State Space model class.\n\n    Represents a discrete-time system:\n    x[k+1] = A x[k] + B u[k]\n    y[k]   = C x[k] + D u[k]\n    \"\"\"\n\n    def __init__(\n        self,\n        A: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        B: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        C: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        D: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        Ts: float = 1.0,\n        nx: int = 0,\n        nu: int = 1,\n        ny: int = 1,\n        nk: int = 0,\n        input_names: Optional[List[str]] = None,\n        output_names: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize StateSpaceModel.\n\n        Args:\n            A: State transition matrix.\n            B: Input matrix.\n            C: Output matrix.\n            D: Feedthrough matrix.\n            Ts: Sampling time.\n            nx: Number of states (used if A is None).\n            nu: Number of inputs (used if B is None).\n            ny: Number of outputs (used if C is None).\n            nk: Input delay (samples).\n            input_names: List of input names.\n            output_names: List of output names.\n        \"\"\"\n        if input_names is None:\n            input_names = []\n        if output_names is None:\n            output_names = []\n        super().__init__(Ts=Ts, input_names=input_names, output_names=output_names)\n\n        self.nk = nk\n\n        # set A matrix and number of states\n        if A is not None:\n            self.A = np.array(A, dtype=float)\n            self.nx = self.A.shape[0]\n        else:\n            self.nx = nx\n            self.A = np.zeros((self.nx, self.nx))\n\n        # set B matrix and number of inputs\n        if B is not None:\n            self.B = np.array(B, dtype=float).reshape(self.nx, -1)\n            self.nu = self.B.shape[1]\n        else:\n            self.nu = nu\n            self.B = np.zeros((self.nx, self.nu))\n\n        # set C matrix and number of outputs\n        if C is not None:\n            self.C = np.array(C, dtype=float).reshape(-1, self.nx)\n            self.ny = self.C.shape[0]\n        else:\n            self.ny = ny\n            self.C = np.zeros((self.ny, self.nx))\n\n        if D is not None:\n            self.D = np.array(D, dtype=float).reshape(self.ny, self.nu)\n        else:\n            self.D = np.zeros((self.ny, self.nu))\n\n        self.x_init = np.zeros((self.nx, 1))\n        self.cov: Optional[np.ndarray] = None\n        self.logger = logging.getLogger(__name__)\n\n    def vectorize(self, include_init_state: bool = True) -&gt; np.ndarray:\n        \"\"\"Vectorize model parameters.\"\"\"\n        theta = np.vstack(\n            [\n                self.A.reshape(-1, 1),\n                self.B.reshape(-1, 1),\n                self.C.reshape(-1, 1),\n                self.D.reshape(-1, 1),\n            ]\n        )\n        if include_init_state:\n            if self.x_init is None:\n                self.x_init = np.zeros((self.nx, 1))\n            theta = np.vstack([theta, self.x_init.reshape(-1, 1)])\n\n        return np.array(theta).ravel()\n\n    def reshape(self, theta: np.ndarray, include_init_state: bool = True) -&gt; None:\n        \"\"\"Update model parameters from vector.\"\"\"\n        nx = self.nx\n        nu = self.nu\n        ny = self.ny\n\n        na = nx * nx\n        nb = nx * nu\n        nc = ny * nx\n        nd = ny * nu\n\n        self.A = theta[:na].reshape(nx, nx)\n        self.B = theta[na : na + nb].reshape(nx, nu)\n        self.C = theta[na + nb : na + nb + nc].reshape(ny, nx)\n        self.D = theta[na + nb + nc : na + nb + nc + nd].reshape(ny, nu)\n\n        if include_init_state:\n            self.x_init = theta[na + nb + nc + nd :].reshape(nx, 1)\n\n    def simulate(self, u: np.ndarray, uncertainty: bool = False) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Simulate the model.\"\"\"\n        u = np.atleast_2d(u)\n        if u.shape[0] != self.nu:\n            # Try transposing if dimensions mismatch but match after transpose\n            if u.shape[1] == self.nu:\n                u = u.T\n\n        # Ensure u is (nu, N) for evaluate_state_space?\n        # evaluate_state_space expects u as (nu, N) or (N, nu)?\n        # Looking at math.py:\n        # u : (nu, N) array\n\n        # But wait, usually we pass (N, nu) or (N,) to simulate.\n        # The original code did: u = u.reshape(self.nu, -1)\n        # This implies it expects input to be flattened into (nu, N) somehow?\n        # If u is (N, nu), reshape(nu, -1) might scramble data if not careful.\n        # If u is (N, 1), reshape(1, -1) -&gt; (1, N). Correct.\n        # If u is (N, 2), reshape(2, -1) -&gt; (2, N). Correct ONLY if data is row-major and we want to split rows?\n        # No, if u is (N, nu), we want (nu, N). u.T is better.\n\n        # Let's assume u is (N, nu) or (N,).\n        if u.ndim == 1:\n            u = u.reshape(1, -1)  # (1, N)\n        elif u.shape[0] == self.nu:\n            pass  # Already (nu, N)\n        elif u.shape[1] == self.nu:\n            u = u.T  # Convert (N, nu) to (nu, N)\n        else:\n            # Fallback to original behavior but it's risky\n            u = u.reshape(self.nu, -1)\n\n        # Apply input delay\n        if self.nk &gt; 0:\n            # Prepend nk columns of zeros\n            zeros = np.zeros((self.nu, self.nk))\n            u = np.hstack((zeros, u[:, : -self.nk]))\n\n        u = np.ascontiguousarray(u)\n\n        if self.x_init is None:\n            x1 = np.zeros((self.nx, 1))\n        else:\n            x1 = self.x_init\n\n        y = evaluate_state_space(\n            self.A.astype(np.float64),\n            self.B.astype(np.float64),\n            self.C.astype(np.float64),\n            self.D.astype(np.float64),\n            u.astype(np.float64),\n            x1.astype(np.float64),\n        )\n        # y is returned as (N, ny) from evaluate_state_space\n\n        if uncertainty:\n            if not hasattr(self, \"cov\") or self.cov is None:\n                return y, None\n\n            # Ensure u is correct shape for closure\n            # We need to pass the original u (or close to it) to the closure\n            # But we modified u above.\n            # Let's just use the modified u, but we need to be careful about recursion.\n            # Actually, evaluate_state_space is static/external, so we can just wrap that?\n            # No, _propagate_uncertainty perturbs parameters (A, B, C, D) and calls func().\n            # So func() needs to call simulate() or evaluate_state_space() with NEW parameters.\n            # Calling self.simulate(..., uncertainty=False) is the right way.\n            # But we need to pass the original input format if possible, or the processed one?\n            # If we pass processed u (nu, N), simulate might try to process it again.\n            # If we pass (nu, N), simulate checks:\n            # if u.shape[0] == self.nu: pass. So it works.\n\n            u_for_closure = u.copy()\n\n            def func():\n                return self.simulate(u_for_closure, uncertainty=False).ravel()\n\n            y_std = self._propagate_uncertainty(func)\n            y_std = y_std.reshape(y.shape)\n            return y, y_std\n\n        return y\n\n    def frequency_response(\n        self, omega: np.ndarray = np.logspace(-3, 2), uncertainty: bool = False\n    ) -&gt; Union[\n        Tuple[np.ndarray, np.ndarray],\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n    ]:\n        \"\"\"Compute frequency response.\"\"\"\n        A = self.A\n        B = self.B\n        C = self.C\n        D = self.D\n\n        z = np.exp(1j * omega * self.Ts)\n        H = []\n\n        # H(z) = C(zI - A)^-1 B + D\n        # This loop is slow for many frequencies.\n        # Could use scipy.signal.freqresp if we convert to ss.\n\n        eye = np.eye(A.shape[0])\n        for z_ in z:\n            # Solve (zI - A) X = B -&gt; X = (zI - A)^-1 B\n            # Then H = C X + D\n            try:\n                X = scipy.linalg.solve(z_ * eye - A, B)\n                h = C @ X + D\n            except scipy.linalg.LinAlgError:\n                h = np.full((self.ny, self.nu), np.nan)\n            H.append(h)\n\n        H_arr = np.array(H)\n\n        if uncertainty:\n            if not hasattr(self, \"cov\") or self.cov is None:\n                return omega, H_arr, None, None\n\n            def func():\n                _, H_ = self.frequency_response(omega, uncertainty=False)\n                H_ = H_.ravel()\n                return np.concatenate([np.abs(H_), np.unwrap(np.angle(H_))])\n\n            std = self._propagate_uncertainty(func)\n            n = H_arr.size\n            mag_std = std[:n].reshape(H_arr.shape)\n            phase_std = std[n:].reshape(H_arr.shape)\n\n            return omega, H_arr, mag_std, phase_std\n\n        return omega, H_arr\n        # Could use scipy.signal.freqresp if we convert to ss.\n\n        eye = np.eye(A.shape[0])\n        for z_ in z:\n            # Solve (zI - A) X = B -&gt; X = (zI - A)^-1 B\n            # Then H = C X + D\n            try:\n                X = scipy.linalg.solve(z_ * eye - A, B)\n                h = C @ X + D\n            except scipy.linalg.LinAlgError:\n                h = np.full((self.ny, self.nu), np.nan)\n            H.append(h)\n\n        return omega, np.array(H)\n\n    @classmethod\n    def from_PT1(cls, K: float, tauC: float, Ts: float = 1.0) -&gt; \"StateSpaceModel\":\n        \"\"\"Create from PT1 parameters.\"\"\"\n        t = 2 * tauC\n        tt = 1 / (Ts + t)\n        b = K * Ts * tt\n        a = (Ts - t) * tt\n\n        B = [[(1 - a) * b]]\n        D = [[b]]\n\n        A = [[-a]]\n        C = [[1]]\n\n        mod = cls(A=A, B=B, C=C, D=D, Ts=Ts, nx=1)\n\n        return mod\n\n    def to_ss(self, continuous: bool = False, method: str = \"bilinear\") -&gt; scipy.signal.StateSpace:\n        \"\"\"\n        Convert the model to a Scipy StateSpace representation.\n\n        Args:\n            continuous: If True, convert to a continuous-time system.\n            method: The method to use for discrete-to-continuous conversion if `continuous=True`.\n                    Options: 'bilinear' (Tustin), 'euler' (Forward Euler).\n                    Default is 'bilinear'.\n\n        Returns:\n            scipy.signal.StateSpace: The state-space representation.\n        \"\"\"\n        if continuous:\n            A, B, C, D = self._d2c(self.A, self.B, self.C, self.D, self.Ts, method=method)\n            sys = scipy.signal.StateSpace(A, B, C, D)\n        else:\n            sys = scipy.signal.StateSpace(self.A, self.B, self.C, self.D, dt=self.Ts)\n        return sys\n\n    def d2c(self, method: str = \"bilinear\") -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Convert the discrete-time model matrices to continuous-time matrices.\n\n        Args:\n            method: The method to use for conversion.\n                    Options: 'bilinear' (Tustin), 'euler' (Forward Euler).\n                    Default is 'bilinear'.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: The continuous-time matrices (Ac, Bc, Cc, Dc).\n        \"\"\"\n        return self._d2c(self.A, self.B, self.C, self.D, self.Ts, method=method)\n\n    @staticmethod\n    def _d2c(\n        A: np.ndarray,\n        B: np.ndarray,\n        C: np.ndarray,\n        D: np.ndarray,\n        Ts: float,\n        method: str = \"bilinear\",\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        if method == \"bilinear\":\n            return StateSpaceModel._d2c_bilinear(A, B, C, D, Ts)\n        else:\n            return StateSpaceModel._d2c_euler(A, B, C, D, Ts)\n\n    @staticmethod\n    def _d2c_bilinear(A: np.ndarray, B: np.ndarray, C: np.ndarray, D: np.ndarray, Ts: float):\n        eye = np.eye(*A.shape)\n        AI = scipy.linalg.inv(A + eye)\n        A_ = 2.0 / Ts * (A - eye) @ AI\n        B_ = 2.0 / Ts * (eye - (A - eye) @ AI) @ B\n        C_ = C @ AI\n        D_ = D - C @ AI @ B\n        return A_, B_, C_, D_\n\n    @staticmethod\n    def _d2c_euler(A: np.ndarray, B: np.ndarray, C: np.ndarray, D: np.ndarray, Ts: float):\n        A_ = (A - np.eye(*A.shape)) / Ts\n        B_ = B / Ts\n        C_ = C\n        D_ = D\n        return A_, B_, C_, D_\n\n    def to_tf(self, continuous: bool = False, method: str = \"bilinear\") -&gt; scipy.signal.TransferFunction:\n        \"\"\"\n        Convert the model to a Scipy TransferFunction representation.\n\n        Args:\n            continuous: If True, convert to a continuous-time system.\n            method: The method to use for discrete-to-continuous conversion if `continuous=True`.\n                    Options: 'bilinear' (Tustin), 'euler' (Forward Euler).\n                    Default is 'bilinear'.\n\n        Returns:\n            scipy.signal.TransferFunction: The transfer function representation.\n        \"\"\"\n        sys = self.to_ss(continuous=continuous, method=method)\n        return sys.to_tf()\n\n    def to_zpk(self, continuous=False, method=\"bilinear\"):\n        sys = self.to_ss(continuous=continuous, method=method)\n        return sys.to_zpk()\n\n    def to_controllable_form(self):\n        tf = self.to_tf()\n        ss = tf.to_ss()\n        return StateSpaceModel(A=ss.A, B=ss.B, C=ss.C, D=ss.D, Ts=self.Ts)\n\n    def reduce_order(self, n: int) -&gt; Tuple[\"StateSpaceModel\", np.ndarray]:\n        \"\"\"\n        Perform order reduction using balanced truncation.\n\n        Args:\n            n: New (reduced) model order.\n\n        Returns:\n            Tuple[StateSpaceModel, np.ndarray]: (Reduced model, Hankel singular values)\n        \"\"\"\n        A = self.A\n        B = self.B\n        C = self.C\n\n        if n &gt; A.shape[0]:\n            raise ValueError(f\"New model order has to be &lt;= {A.shape[0]} but is {n}\")\n\n        # controllability gramian\n        W_c = scipy.linalg.solve_discrete_lyapunov(A, B @ B.T)\n\n        # observability gramian\n        W_o = scipy.linalg.solve_discrete_lyapunov(A.T, C.T @ C)\n\n        # controllability matrix\n        S = scipy.linalg.cholesky(W_c)\n\n        # observability matrix\n        R = scipy.linalg.cholesky(W_o)\n\n        U, s, V = scipy.linalg.svd(S @ R.T)\n\n        # truncation\n        U1 = U[:, :n]\n        V1 = V[:, :n]\n\n        # balancing-free square root algorithm\n        W, X = scipy.linalg.qr(S.T @ U1, mode=\"economic\")\n        Z, Y = scipy.linalg.qr(R.T @ V1, mode=\"economic\")\n        UE, sE, VE = scipy.linalg.svd(Z.T @ W)\n\n        SigmaE = np.diag(1 / sE)\n        T_l = np.sqrt(SigmaE) @ UE.T @ Z.T\n        T_r = W @ VE @ np.sqrt(SigmaE)\n\n        # apply transformation\n        A_ = T_l @ A @ T_r\n        B_ = T_l @ B\n        C_ = C @ T_r\n\n        return StateSpaceModel(A=A_, B=B_, C=C_, D=self.D, Ts=self.Ts), s\n\n    @classmethod\n    def from_scipy(cls, mod: Any) -&gt; \"StateSpaceModel\":\n        if hasattr(mod, \"ss\"):\n            ss = mod.ss()\n        elif isinstance(mod, scipy.signal.StateSpace):\n            ss = mod\n        else:\n            ss = mod.to_ss()\n\n        # Check for dt\n        dt = getattr(ss, \"dt\", 1.0)\n        if dt is None:\n            dt = 1.0\n\n        mod_out = cls(A=ss.A, B=ss.B, C=ss.C, D=ss.D, Ts=dt)\n        return mod_out\n\n    @classmethod\n    def from_fir(cls, mod: Any) -&gt; \"StateSpaceModel\":\n        \"\"\"Create state-space model from FIR model (PolynomialModel).\"\"\"\n        nk = getattr(mod, \"nk\", 0)\n        b_coeffs = getattr(mod, \"b\", np.array([1.0]))\n\n        b = np.vstack([np.zeros((nk, 1)), b_coeffs.reshape(-1, 1)])\n        n = b.ravel().shape[0] - 1\n\n        if n &lt; 1:\n            # Trivial case\n            return cls(A=[[0]], B=[[0]], C=[[0]], D=[[b[0, 0]]], Ts=mod.Ts)\n\n        A = np.diag(np.ones((n - 1,)), k=-1)\n        B = np.zeros((n, 1))\n        B[0] = 1.0\n        C = b[1:].reshape(1, -1)\n        D = b[0]\n        mod_out = cls(\n            A=A,\n            B=B,\n            C=C,\n            D=D,\n            Ts=mod.Ts,\n            input_names=mod.input_names,\n            output_names=mod.output_names,\n        )\n        return mod_out\n\n    def to_json(self, filename: Optional[str] = None) -&gt; str:\n        data = {}\n        data[\"A\"] = self.A.tolist()\n        data[\"B\"] = self.B.tolist()\n        data[\"C\"] = self.C.tolist()\n        data[\"D\"] = self.D.tolist()\n        data[\"Ts\"] = self.Ts\n        try:\n            data[\"info\"] = str(self.info)\n        except AttributeError:\n            data[\"info\"] = \"\"\n\n        data[\"nx\"] = self.nx\n        data[\"nu\"] = self.nu\n        data[\"ny\"] = self.ny\n        data[\"nk\"] = self.nk\n        data[\"input_names\"] = self.input_names\n        data[\"output_names\"] = self.output_names\n\n        if self.x_init is not None:\n            data[\"x_init\"] = self.x_init.tolist()\n\n        if self.cov is not None:\n            data[\"cov\"] = self.cov.tolist()\n\n        if filename is not None:\n            with open(filename, \"w\") as f:\n                json.dump(data, f, indent=4)\n            return \"\"\n\n        return json.dumps(data, indent=4)\n\n    @classmethod\n    def from_json(cls, filename: str) -&gt; \"StateSpaceModel\":\n        with open(filename) as f:\n            data = json.load(f)\n        mod = StateSpaceModel(\n            A=data[\"A\"],\n            B=data[\"B\"],\n            C=data[\"C\"],\n            D=data[\"D\"],\n            Ts=data[\"Ts\"],\n            nx=data.get(\"nx\", 0),\n            nu=data.get(\"nu\", 1),\n            ny=data.get(\"ny\", 1),\n            nk=data.get(\"nk\", 0),\n            input_names=data.get(\"input_names\"),\n            output_names=data.get(\"output_names\"),\n        )\n        if \"info\" in data:\n            mod.info = data[\"info\"]\n\n        if \"x_init\" in data:\n            mod.x_init = np.array(data[\"x_init\"])\n\n        if \"cov\" in data:\n            mod.cov = np.array(data[\"cov\"])\n\n        return mod\n\n    def __repr__(self) -&gt; str:\n        s = f\"StateSpaceModel with Ts={self.Ts}\\n\"\n        s += f\"input(s): {self.input_names}\\n\"\n        s += f\"output(s): {self.output_names}\\n\"\n        s += f\"A:\\n{self.A}\\n\"\n        s += f\"B:\\n{self.B}\\n\"\n        s += f\"C:\\n{self.C}\\n\"\n        s += f\"D:\\n{self.D}\\n\"\n        return s\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.__init__","title":"<code>__init__(A=None, B=None, C=None, D=None, Ts=1.0, nx=0, nu=1, ny=1, nk=0, input_names=None, output_names=None)</code>","text":"<p>Initialize StateSpaceModel.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Optional[Union[ndarray, List[List[float]]]]</code> <p>State transition matrix.</p> <code>None</code> <code>B</code> <code>Optional[Union[ndarray, List[List[float]]]]</code> <p>Input matrix.</p> <code>None</code> <code>C</code> <code>Optional[Union[ndarray, List[List[float]]]]</code> <p>Output matrix.</p> <code>None</code> <code>D</code> <code>Optional[Union[ndarray, List[List[float]]]]</code> <p>Feedthrough matrix.</p> <code>None</code> <code>Ts</code> <code>float</code> <p>Sampling time.</p> <code>1.0</code> <code>nx</code> <code>int</code> <p>Number of states (used if A is None).</p> <code>0</code> <code>nu</code> <code>int</code> <p>Number of inputs (used if B is None).</p> <code>1</code> <code>ny</code> <code>int</code> <p>Number of outputs (used if C is None).</p> <code>1</code> <code>nk</code> <code>int</code> <p>Input delay (samples).</p> <code>0</code> <code>input_names</code> <code>Optional[List[str]]</code> <p>List of input names.</p> <code>None</code> <code>output_names</code> <code>Optional[List[str]]</code> <p>List of output names.</p> <code>None</code> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def __init__(\n    self,\n    A: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    B: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    C: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    D: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    Ts: float = 1.0,\n    nx: int = 0,\n    nu: int = 1,\n    ny: int = 1,\n    nk: int = 0,\n    input_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n):\n    \"\"\"\n    Initialize StateSpaceModel.\n\n    Args:\n        A: State transition matrix.\n        B: Input matrix.\n        C: Output matrix.\n        D: Feedthrough matrix.\n        Ts: Sampling time.\n        nx: Number of states (used if A is None).\n        nu: Number of inputs (used if B is None).\n        ny: Number of outputs (used if C is None).\n        nk: Input delay (samples).\n        input_names: List of input names.\n        output_names: List of output names.\n    \"\"\"\n    if input_names is None:\n        input_names = []\n    if output_names is None:\n        output_names = []\n    super().__init__(Ts=Ts, input_names=input_names, output_names=output_names)\n\n    self.nk = nk\n\n    # set A matrix and number of states\n    if A is not None:\n        self.A = np.array(A, dtype=float)\n        self.nx = self.A.shape[0]\n    else:\n        self.nx = nx\n        self.A = np.zeros((self.nx, self.nx))\n\n    # set B matrix and number of inputs\n    if B is not None:\n        self.B = np.array(B, dtype=float).reshape(self.nx, -1)\n        self.nu = self.B.shape[1]\n    else:\n        self.nu = nu\n        self.B = np.zeros((self.nx, self.nu))\n\n    # set C matrix and number of outputs\n    if C is not None:\n        self.C = np.array(C, dtype=float).reshape(-1, self.nx)\n        self.ny = self.C.shape[0]\n    else:\n        self.ny = ny\n        self.C = np.zeros((self.ny, self.nx))\n\n    if D is not None:\n        self.D = np.array(D, dtype=float).reshape(self.ny, self.nu)\n    else:\n        self.D = np.zeros((self.ny, self.nu))\n\n    self.x_init = np.zeros((self.nx, 1))\n    self.cov: Optional[np.ndarray] = None\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.d2c","title":"<code>d2c(method='bilinear')</code>","text":"<p>Convert the discrete-time model matrices to continuous-time matrices.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to use for conversion.     Options: 'bilinear' (Tustin), 'euler' (Forward Euler).     Default is 'bilinear'.</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: The continuous-time matrices (Ac, Bc, Cc, Dc).</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def d2c(self, method: str = \"bilinear\") -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Convert the discrete-time model matrices to continuous-time matrices.\n\n    Args:\n        method: The method to use for conversion.\n                Options: 'bilinear' (Tustin), 'euler' (Forward Euler).\n                Default is 'bilinear'.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: The continuous-time matrices (Ac, Bc, Cc, Dc).\n    \"\"\"\n    return self._d2c(self.A, self.B, self.C, self.D, self.Ts, method=method)\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.frequency_response","title":"<code>frequency_response(omega=np.logspace(-3, 2), uncertainty=False)</code>","text":"<p>Compute frequency response.</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def frequency_response(\n    self, omega: np.ndarray = np.logspace(-3, 2), uncertainty: bool = False\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray],\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n]:\n    \"\"\"Compute frequency response.\"\"\"\n    A = self.A\n    B = self.B\n    C = self.C\n    D = self.D\n\n    z = np.exp(1j * omega * self.Ts)\n    H = []\n\n    # H(z) = C(zI - A)^-1 B + D\n    # This loop is slow for many frequencies.\n    # Could use scipy.signal.freqresp if we convert to ss.\n\n    eye = np.eye(A.shape[0])\n    for z_ in z:\n        # Solve (zI - A) X = B -&gt; X = (zI - A)^-1 B\n        # Then H = C X + D\n        try:\n            X = scipy.linalg.solve(z_ * eye - A, B)\n            h = C @ X + D\n        except scipy.linalg.LinAlgError:\n            h = np.full((self.ny, self.nu), np.nan)\n        H.append(h)\n\n    H_arr = np.array(H)\n\n    if uncertainty:\n        if not hasattr(self, \"cov\") or self.cov is None:\n            return omega, H_arr, None, None\n\n        def func():\n            _, H_ = self.frequency_response(omega, uncertainty=False)\n            H_ = H_.ravel()\n            return np.concatenate([np.abs(H_), np.unwrap(np.angle(H_))])\n\n        std = self._propagate_uncertainty(func)\n        n = H_arr.size\n        mag_std = std[:n].reshape(H_arr.shape)\n        phase_std = std[n:].reshape(H_arr.shape)\n\n        return omega, H_arr, mag_std, phase_std\n\n    return omega, H_arr\n    # Could use scipy.signal.freqresp if we convert to ss.\n\n    eye = np.eye(A.shape[0])\n    for z_ in z:\n        # Solve (zI - A) X = B -&gt; X = (zI - A)^-1 B\n        # Then H = C X + D\n        try:\n            X = scipy.linalg.solve(z_ * eye - A, B)\n            h = C @ X + D\n        except scipy.linalg.LinAlgError:\n            h = np.full((self.ny, self.nu), np.nan)\n        H.append(h)\n\n    return omega, np.array(H)\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.from_PT1","title":"<code>from_PT1(K, tauC, Ts=1.0)</code>  <code>classmethod</code>","text":"<p>Create from PT1 parameters.</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>@classmethod\ndef from_PT1(cls, K: float, tauC: float, Ts: float = 1.0) -&gt; \"StateSpaceModel\":\n    \"\"\"Create from PT1 parameters.\"\"\"\n    t = 2 * tauC\n    tt = 1 / (Ts + t)\n    b = K * Ts * tt\n    a = (Ts - t) * tt\n\n    B = [[(1 - a) * b]]\n    D = [[b]]\n\n    A = [[-a]]\n    C = [[1]]\n\n    mod = cls(A=A, B=B, C=C, D=D, Ts=Ts, nx=1)\n\n    return mod\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.from_fir","title":"<code>from_fir(mod)</code>  <code>classmethod</code>","text":"<p>Create state-space model from FIR model (PolynomialModel).</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>@classmethod\ndef from_fir(cls, mod: Any) -&gt; \"StateSpaceModel\":\n    \"\"\"Create state-space model from FIR model (PolynomialModel).\"\"\"\n    nk = getattr(mod, \"nk\", 0)\n    b_coeffs = getattr(mod, \"b\", np.array([1.0]))\n\n    b = np.vstack([np.zeros((nk, 1)), b_coeffs.reshape(-1, 1)])\n    n = b.ravel().shape[0] - 1\n\n    if n &lt; 1:\n        # Trivial case\n        return cls(A=[[0]], B=[[0]], C=[[0]], D=[[b[0, 0]]], Ts=mod.Ts)\n\n    A = np.diag(np.ones((n - 1,)), k=-1)\n    B = np.zeros((n, 1))\n    B[0] = 1.0\n    C = b[1:].reshape(1, -1)\n    D = b[0]\n    mod_out = cls(\n        A=A,\n        B=B,\n        C=C,\n        D=D,\n        Ts=mod.Ts,\n        input_names=mod.input_names,\n        output_names=mod.output_names,\n    )\n    return mod_out\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.reduce_order","title":"<code>reduce_order(n)</code>","text":"<p>Perform order reduction using balanced truncation.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>New (reduced) model order.</p> required <p>Returns:</p> Type Description <code>Tuple[StateSpaceModel, ndarray]</code> <p>Tuple[StateSpaceModel, np.ndarray]: (Reduced model, Hankel singular values)</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def reduce_order(self, n: int) -&gt; Tuple[\"StateSpaceModel\", np.ndarray]:\n    \"\"\"\n    Perform order reduction using balanced truncation.\n\n    Args:\n        n: New (reduced) model order.\n\n    Returns:\n        Tuple[StateSpaceModel, np.ndarray]: (Reduced model, Hankel singular values)\n    \"\"\"\n    A = self.A\n    B = self.B\n    C = self.C\n\n    if n &gt; A.shape[0]:\n        raise ValueError(f\"New model order has to be &lt;= {A.shape[0]} but is {n}\")\n\n    # controllability gramian\n    W_c = scipy.linalg.solve_discrete_lyapunov(A, B @ B.T)\n\n    # observability gramian\n    W_o = scipy.linalg.solve_discrete_lyapunov(A.T, C.T @ C)\n\n    # controllability matrix\n    S = scipy.linalg.cholesky(W_c)\n\n    # observability matrix\n    R = scipy.linalg.cholesky(W_o)\n\n    U, s, V = scipy.linalg.svd(S @ R.T)\n\n    # truncation\n    U1 = U[:, :n]\n    V1 = V[:, :n]\n\n    # balancing-free square root algorithm\n    W, X = scipy.linalg.qr(S.T @ U1, mode=\"economic\")\n    Z, Y = scipy.linalg.qr(R.T @ V1, mode=\"economic\")\n    UE, sE, VE = scipy.linalg.svd(Z.T @ W)\n\n    SigmaE = np.diag(1 / sE)\n    T_l = np.sqrt(SigmaE) @ UE.T @ Z.T\n    T_r = W @ VE @ np.sqrt(SigmaE)\n\n    # apply transformation\n    A_ = T_l @ A @ T_r\n    B_ = T_l @ B\n    C_ = C @ T_r\n\n    return StateSpaceModel(A=A_, B=B_, C=C_, D=self.D, Ts=self.Ts), s\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.reshape","title":"<code>reshape(theta, include_init_state=True)</code>","text":"<p>Update model parameters from vector.</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def reshape(self, theta: np.ndarray, include_init_state: bool = True) -&gt; None:\n    \"\"\"Update model parameters from vector.\"\"\"\n    nx = self.nx\n    nu = self.nu\n    ny = self.ny\n\n    na = nx * nx\n    nb = nx * nu\n    nc = ny * nx\n    nd = ny * nu\n\n    self.A = theta[:na].reshape(nx, nx)\n    self.B = theta[na : na + nb].reshape(nx, nu)\n    self.C = theta[na + nb : na + nb + nc].reshape(ny, nx)\n    self.D = theta[na + nb + nc : na + nb + nc + nd].reshape(ny, nu)\n\n    if include_init_state:\n        self.x_init = theta[na + nb + nc + nd :].reshape(nx, 1)\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.simulate","title":"<code>simulate(u, uncertainty=False)</code>","text":"<p>Simulate the model.</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def simulate(self, u: np.ndarray, uncertainty: bool = False) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Simulate the model.\"\"\"\n    u = np.atleast_2d(u)\n    if u.shape[0] != self.nu:\n        # Try transposing if dimensions mismatch but match after transpose\n        if u.shape[1] == self.nu:\n            u = u.T\n\n    # Ensure u is (nu, N) for evaluate_state_space?\n    # evaluate_state_space expects u as (nu, N) or (N, nu)?\n    # Looking at math.py:\n    # u : (nu, N) array\n\n    # But wait, usually we pass (N, nu) or (N,) to simulate.\n    # The original code did: u = u.reshape(self.nu, -1)\n    # This implies it expects input to be flattened into (nu, N) somehow?\n    # If u is (N, nu), reshape(nu, -1) might scramble data if not careful.\n    # If u is (N, 1), reshape(1, -1) -&gt; (1, N). Correct.\n    # If u is (N, 2), reshape(2, -1) -&gt; (2, N). Correct ONLY if data is row-major and we want to split rows?\n    # No, if u is (N, nu), we want (nu, N). u.T is better.\n\n    # Let's assume u is (N, nu) or (N,).\n    if u.ndim == 1:\n        u = u.reshape(1, -1)  # (1, N)\n    elif u.shape[0] == self.nu:\n        pass  # Already (nu, N)\n    elif u.shape[1] == self.nu:\n        u = u.T  # Convert (N, nu) to (nu, N)\n    else:\n        # Fallback to original behavior but it's risky\n        u = u.reshape(self.nu, -1)\n\n    # Apply input delay\n    if self.nk &gt; 0:\n        # Prepend nk columns of zeros\n        zeros = np.zeros((self.nu, self.nk))\n        u = np.hstack((zeros, u[:, : -self.nk]))\n\n    u = np.ascontiguousarray(u)\n\n    if self.x_init is None:\n        x1 = np.zeros((self.nx, 1))\n    else:\n        x1 = self.x_init\n\n    y = evaluate_state_space(\n        self.A.astype(np.float64),\n        self.B.astype(np.float64),\n        self.C.astype(np.float64),\n        self.D.astype(np.float64),\n        u.astype(np.float64),\n        x1.astype(np.float64),\n    )\n    # y is returned as (N, ny) from evaluate_state_space\n\n    if uncertainty:\n        if not hasattr(self, \"cov\") or self.cov is None:\n            return y, None\n\n        # Ensure u is correct shape for closure\n        # We need to pass the original u (or close to it) to the closure\n        # But we modified u above.\n        # Let's just use the modified u, but we need to be careful about recursion.\n        # Actually, evaluate_state_space is static/external, so we can just wrap that?\n        # No, _propagate_uncertainty perturbs parameters (A, B, C, D) and calls func().\n        # So func() needs to call simulate() or evaluate_state_space() with NEW parameters.\n        # Calling self.simulate(..., uncertainty=False) is the right way.\n        # But we need to pass the original input format if possible, or the processed one?\n        # If we pass processed u (nu, N), simulate might try to process it again.\n        # If we pass (nu, N), simulate checks:\n        # if u.shape[0] == self.nu: pass. So it works.\n\n        u_for_closure = u.copy()\n\n        def func():\n            return self.simulate(u_for_closure, uncertainty=False).ravel()\n\n        y_std = self._propagate_uncertainty(func)\n        y_std = y_std.reshape(y.shape)\n        return y, y_std\n\n    return y\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.to_ss","title":"<code>to_ss(continuous=False, method='bilinear')</code>","text":"<p>Convert the model to a Scipy StateSpace representation.</p> <p>Parameters:</p> Name Type Description Default <code>continuous</code> <code>bool</code> <p>If True, convert to a continuous-time system.</p> <code>False</code> <code>method</code> <code>str</code> <p>The method to use for discrete-to-continuous conversion if <code>continuous=True</code>.     Options: 'bilinear' (Tustin), 'euler' (Forward Euler).     Default is 'bilinear'.</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>StateSpace</code> <p>scipy.signal.StateSpace: The state-space representation.</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def to_ss(self, continuous: bool = False, method: str = \"bilinear\") -&gt; scipy.signal.StateSpace:\n    \"\"\"\n    Convert the model to a Scipy StateSpace representation.\n\n    Args:\n        continuous: If True, convert to a continuous-time system.\n        method: The method to use for discrete-to-continuous conversion if `continuous=True`.\n                Options: 'bilinear' (Tustin), 'euler' (Forward Euler).\n                Default is 'bilinear'.\n\n    Returns:\n        scipy.signal.StateSpace: The state-space representation.\n    \"\"\"\n    if continuous:\n        A, B, C, D = self._d2c(self.A, self.B, self.C, self.D, self.Ts, method=method)\n        sys = scipy.signal.StateSpace(A, B, C, D)\n    else:\n        sys = scipy.signal.StateSpace(self.A, self.B, self.C, self.D, dt=self.Ts)\n    return sys\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.to_tf","title":"<code>to_tf(continuous=False, method='bilinear')</code>","text":"<p>Convert the model to a Scipy TransferFunction representation.</p> <p>Parameters:</p> Name Type Description Default <code>continuous</code> <code>bool</code> <p>If True, convert to a continuous-time system.</p> <code>False</code> <code>method</code> <code>str</code> <p>The method to use for discrete-to-continuous conversion if <code>continuous=True</code>.     Options: 'bilinear' (Tustin), 'euler' (Forward Euler).     Default is 'bilinear'.</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>TransferFunction</code> <p>scipy.signal.TransferFunction: The transfer function representation.</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def to_tf(self, continuous: bool = False, method: str = \"bilinear\") -&gt; scipy.signal.TransferFunction:\n    \"\"\"\n    Convert the model to a Scipy TransferFunction representation.\n\n    Args:\n        continuous: If True, convert to a continuous-time system.\n        method: The method to use for discrete-to-continuous conversion if `continuous=True`.\n                Options: 'bilinear' (Tustin), 'euler' (Forward Euler).\n                Default is 'bilinear'.\n\n    Returns:\n        scipy.signal.TransferFunction: The transfer function representation.\n    \"\"\"\n    sys = self.to_ss(continuous=continuous, method=method)\n    return sys.to_tf()\n</code></pre>"},{"location":"api/statespacemodel/#llsi.statespacemodel.StateSpaceModel.vectorize","title":"<code>vectorize(include_init_state=True)</code>","text":"<p>Vectorize model parameters.</p> Source code in <code>src/llsi/statespacemodel.py</code> <pre><code>def vectorize(self, include_init_state: bool = True) -&gt; np.ndarray:\n    \"\"\"Vectorize model parameters.\"\"\"\n    theta = np.vstack(\n        [\n            self.A.reshape(-1, 1),\n            self.B.reshape(-1, 1),\n            self.C.reshape(-1, 1),\n            self.D.reshape(-1, 1),\n        ]\n    )\n    if include_init_state:\n        if self.x_init is None:\n            self.x_init = np.zeros((self.nx, 1))\n        theta = np.vstack([theta, self.x_init.reshape(-1, 1)])\n\n    return np.array(theta).ravel()\n</code></pre>"},{"location":"api/subspace/","title":"Subspace Identification","text":""},{"location":"api/subspace/#llsi.subspace.SubspaceIdent","title":"<code>llsi.subspace.SubspaceIdent</code>","text":"<p>               Bases: <code>SysIdAlgBase</code></p> <p>Base class for subspace identification methods.</p> Source code in <code>src/llsi/subspace.py</code> <pre><code>class SubspaceIdent(SysIdAlgBase):\n    \"\"\"Base class for subspace identification methods.\"\"\"\n\n    def __init__(\n        self,\n        data: SysIdData,\n        y_name: Union[str, List[str]],\n        u_name: Union[str, List[str]],\n        settings: Optional[Dict[str, Any]] = None,\n    ):\n        if settings is None:\n            settings = {}\n        super().__init__(data, y_name, u_name, settings=settings)\n        self.nu = self.u.shape[1]\n        self.ny = self.y.shape[1]\n\n        # Handle input delay (nk)\n        self.nk = settings.get(\"nk\", 0)\n        if self.nk &gt; 0:\n            # Shift data: y[k] depends on u[k-nk]\n            # We want to pair y[t] with u[t-nk].\n            # So we take y from index nk to end, and u from 0 to end-nk.\n            # This aligns y[nk] with u[0].\n            # Effective length is reduced by nk.\n            if self.y.shape[0] &lt;= self.nk:\n                raise ValueError(f\"Data length {self.y.shape[0]} is too short for delay nk={self.nk}\")\n\n            self.y = self.y[self.nk :, :]\n            self.u = self.u[: -self.nk, :]\n\n    @staticmethod\n    def hankel(x: np.ndarray, n: int) -&gt; np.ndarray:\n        \"\"\"\n        Construct Block Hankel matrix.\n\n        Args:\n            x: Data array (N, n_channels).\n            n: Number of block rows.\n\n        Returns:\n            np.ndarray: Hankel matrix.\n        \"\"\"\n        # x is (N, n_channels)\n        # We want a Hankel matrix with n block rows.\n        # If x has 1 column, it's standard Hankel.\n        # If x has m columns, it's block Hankel.\n\n        N_samples, n_channels = x.shape\n        # Number of block columns\n        # If we have n block rows, we need enough samples.\n        # H is (n * n_channels, N_cols)\n        # N_cols = N_samples - n + 1\n\n        # The original implementation:\n        # n = n // x.shape[1]  &lt;-- This suggests 'n' passed in is total rows, not block rows?\n        # \"n = order[0]; r = (2 * n + 1) * self.nu * self.ny\"\n        # \"Y = self.hankel(self.y, 2 * r)\"\n        # So 'n' passed to hankel is 2*r, which is total rows.\n\n        n_block_rows = n // n_channels\n        if n_block_rows &lt;= 0:\n            raise ValueError(f\"Not enough rows requested for Hankel matrix. n={n}, channels={n_channels}\")\n\n        N_cols = N_samples - n_block_rows + 1\n\n        if N_cols &lt;= 0:\n            raise ValueError(f\"Not enough samples for Hankel matrix. Samples={N_samples}, Block rows={n_block_rows}\")\n\n        # Efficient Hankel construction using stride_tricks or simple loop\n        # Original implementation used loops and lists.\n\n        # Let's stick to a cleaner loop or optimized approach.\n        # H = [x[0:N_cols], x[1:N_cols+1], ..., x[n_block_rows-1:]]\n        # But stacked vertically.\n\n        # Actually, let's follow the original logic to ensure compatibility, but clean it up.\n        # Original:\n        # for i in range(n): (where n is block rows)\n        #   for x_ in x.T: (iterate over channels)\n        #     A.append(x_[i : -n + i])\n\n        # Wait, x_[i : -n + i] ?\n        # if i=0, x_[0 : -n]. Length N-n.\n        # if i=n-1, x_[n-1 : -1]. Length N-n.\n        # This creates columns of length N-n.\n\n        A = []\n        for i in range(n_block_rows):\n            for j in range(n_channels):\n                # Column j of x\n                col = x[:, j]\n                # Slice\n                # We want N_cols elements starting at i.\n                # x[i : i + N_cols]\n                # N_cols = N_samples - n_block_rows + 1\n                # So x[i : i + N_samples - n_block_rows + 1]\n\n                # Original code: x_[i : -n + i]\n                # -n + i = -(n - i).\n                # If n=10, i=0 -&gt; :-10. Length N-10.\n                # If n=10, i=9 -&gt; 9:-1. Length N-10.\n                # So N_cols = N - n.\n                # This drops the last sample?\n                # Usually Hankel uses all samples.\n                # Let's use standard definition: N_cols = N - n + 1.\n\n                # But to match original behavior exactly if needed:\n                # The original code used `x_[i : -n + i]` which excludes the last `n-i` elements?\n                # No, `x_[:-n]` excludes last n elements.\n                # `x_[i : -n + i]` has length `(N - n + i) - i = N - n`.\n                # So it produces `N - n` columns.\n\n                # I will use `N - n_block_rows + 1` columns which is standard.\n                # But wait, if I change dimensions, it might break math downstream.\n                # Let's stick to `N - n_block_rows` columns as per original code implication (N-n).\n\n                end_idx = -n_block_rows + i\n                if end_idx == 0:\n                    segment = col[i:]\n                else:\n                    segment = col[i:end_idx]\n                A.append(segment)\n\n        return np.array(A)\n\n    @staticmethod\n    def enforce_stability(A: np.ndarray, radius: float = 0.99) -&gt; np.ndarray:\n        \"\"\"\n        Enforces stability by scaling eigenvalues of A to be within the unit circle.\n\n        Args:\n            A: The system matrix (n x n).\n            radius: The maximum allowed absolute value for eigenvalues (e.g., 0.99).\n\n        Returns:\n            A_stable: The stabilized system matrix.\n        \"\"\"\n        # Eigenwertzerlegung: A = V * diag(w) * V^-1\n        w, v = scipy.linalg.eig(A)\n\n        # Pr\u00fcfen, ob Eigenwerte au\u00dferhalb des Radius liegen\n        abs_w = np.abs(w)\n        if np.any(abs_w &gt;= 1.0):\n            # Skaliere nur die instabilen Eigenwerte\n            # w_new = w / |w| * radius\n            unstable_mask = abs_w &gt;= 1.0\n            w[unstable_mask] = (w[unstable_mask] / abs_w[unstable_mask]) * radius\n\n            # Rekonstruktion\n            # Wir nehmen den Realteil, da A urspr\u00fcnglich reell war und\n            # numerisches Rauschen kleine imagin\u00e4re Anteile erzeugen kann.\n            A_stable = (v @ np.diag(w) @ np.linalg.inv(v)).real\n            return A_stable\n\n        return A\n\n    def _abcd_state(\n        self, Xf: np.ndarray, s: int, n: int, r: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        # Xf: State sequence (n, N)\n        # s: Number of columns in Hankel matrix (N)\n        # n: Order\n        # r: Window size parameter?\n\n        # Y_ is (n + ny, N-1) ?\n        # Xf[:, 1:s-1] is X_{k+1}\n        # Xf[:, :s-2] is X_k\n\n        # We want to solve:\n        # [X_{k+1}; Y_k] = [A B; C D] [X_k; U_k]\n\n        # Original code:\n        # Y_ = np.vstack((Xf[:, 1 : s - 1], self.y[r : r + s - 2, :].T))\n        # X_ = np.vstack((Xf[:, : s - 2], self.u[r : r + s - 2, :].T))\n\n        # Indices seem to be aligning Xf with y and u.\n        # Xf corresponds to states at times r, r+1, ... ?\n\n        # Let's trust the indices for now but add types.\n\n        # Ensure dimensions match\n        # Xf is (n, s)\n        # We use columns 1 to s-1 (length s-2) for LHS top\n        # We use columns 0 to s-2 (length s-2) for RHS top\n\n        # y and u need to be sliced to match length s-2.\n        # self.y is (N_total, ny).\n        # We take slice [r : r + s - 2].\n\n        Y_ = np.vstack((Xf[:, 1 : s - 1], self.y[r : r + s - 2, :].T))\n        X_ = np.vstack((Xf[:, : s - 2], self.u[r : r + s - 2, :].T))\n\n        lmbd = self.settings.get(\"lambda\", 0.0)\n\n        # Regularized least squares via SVD\n        # Theta = Y_ @ pinv(X_)\n        # X_ = U S Vh\n        # pinv(X_) = Vh.T S^-1 U.T\n        # Regularized: S^-1 -&gt; S / (S^2 + lambda)\n\n        U, s_svd, Vh = scipy.linalg.svd(X_, full_matrices=False)\n\n        # s_svd are singular values\n        # rho = s^2 / (s^2 + lambda) * (1/s) = s / (s^2 + lambda)\n        # Wait, original code:\n        # Sigma = np.diag(1 / s)\n        # rho = np.diag(s**2 / (s**2 + lmbd))\n        # Theta = Y_ @ (Vh.T @ rho @ Sigma @ U.T)\n        # rho @ Sigma = diag( s^2/(s^2+L) * 1/s ) = diag( s / (s^2+L) )\n        # This is Tikhonov regularization on singular values. Correct.\n\n        # Handle division by zero if s_svd has zeros (unlikely with float)\n        s_filt = s_svd / (s_svd**2 + lmbd)\n\n        Theta = Y_ @ (Vh.T @ np.diag(s_filt) @ U.T)\n\n        A = Theta[:n, :n]\n        B = Theta[:n, n:]\n        C = Theta[n:, :n]\n        D = Theta[n:, n:]\n        return A, B, C, D\n\n    def _abcd_observability_matrix(self, U1, U2, L11, L31, Sigma_sqrt, n, r):\n        ny = self.ny\n        nu = self.nu\n\n        Or = U1 @ Sigma_sqrt  # extended observability matrix\n\n        C = Or[:ny, :]\n        # Solve A from Or\n        # Or(1:end-ny, :) A = Or(ny+1:end, :)\n        # A = pinv(Or[:-ny]) @ Or[ny:]\n        A = scipy.linalg.pinv(Or[0:-ny, :]) @ Or[ny:, :]\n\n        # Estimate B and D\n        # This part is complex PO-MOESP logic.\n\n        P = np.split(U2.T, U2.T.shape[1] // ny, axis=1)\n\n        nn = len(P) * P[0].shape[0]\n        rny = r\n        r_blocks = r // ny\n        A_ = np.zeros((nn, ny + n))\n        P1 = np.vstack(P)\n        rr = P[0].shape[0]\n        A_[:, :ny] = P1\n\n        for i in range(1, r_blocks):\n            # Pi_tilda = [P[i], P[i+1], ...]\n            Pi_tilda = np.hstack(P[i:])\n            # Ori = Or[: r - i*ny] ?\n            # Or has r rows (actually r is total rows of Hankel Y, so r rows).\n            # Or is (r, n).\n            # We take top rows.\n            Ori = Or[: rny - (ny * i)]\n            N = Pi_tilda @ Ori\n            A_[(i - 1) * rr : i * rr, ny:] = N\n\n        M = U2.T @ L31 @ np.linalg.inv(L11)\n        Mi = np.split(M, M.shape[1] // nu, axis=1)\n        M = np.vstack(Mi)\n\n        x_, *_ = scipy.linalg.lstsq(A_, M)\n\n        D = x_[:ny, :]\n        B = x_[ny:, :]\n\n        return A, B, C, D\n\n    @staticmethod\n    def lq(A: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute LQ decomposition.\"\"\"\n        # LQ = (QR(A.T)).T = R.T Q.T\n        Q, R = scipy.linalg.qr(A.T, mode=\"economic\")\n        return R.T, Q.T\n</code></pre>"},{"location":"api/subspace/#llsi.subspace.SubspaceIdent.enforce_stability","title":"<code>enforce_stability(A, radius=0.99)</code>  <code>staticmethod</code>","text":"<p>Enforces stability by scaling eigenvalues of A to be within the unit circle.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>The system matrix (n x n).</p> required <code>radius</code> <code>float</code> <p>The maximum allowed absolute value for eigenvalues (e.g., 0.99).</p> <code>0.99</code> <p>Returns:</p> Name Type Description <code>A_stable</code> <code>ndarray</code> <p>The stabilized system matrix.</p> Source code in <code>src/llsi/subspace.py</code> <pre><code>@staticmethod\ndef enforce_stability(A: np.ndarray, radius: float = 0.99) -&gt; np.ndarray:\n    \"\"\"\n    Enforces stability by scaling eigenvalues of A to be within the unit circle.\n\n    Args:\n        A: The system matrix (n x n).\n        radius: The maximum allowed absolute value for eigenvalues (e.g., 0.99).\n\n    Returns:\n        A_stable: The stabilized system matrix.\n    \"\"\"\n    # Eigenwertzerlegung: A = V * diag(w) * V^-1\n    w, v = scipy.linalg.eig(A)\n\n    # Pr\u00fcfen, ob Eigenwerte au\u00dferhalb des Radius liegen\n    abs_w = np.abs(w)\n    if np.any(abs_w &gt;= 1.0):\n        # Skaliere nur die instabilen Eigenwerte\n        # w_new = w / |w| * radius\n        unstable_mask = abs_w &gt;= 1.0\n        w[unstable_mask] = (w[unstable_mask] / abs_w[unstable_mask]) * radius\n\n        # Rekonstruktion\n        # Wir nehmen den Realteil, da A urspr\u00fcnglich reell war und\n        # numerisches Rauschen kleine imagin\u00e4re Anteile erzeugen kann.\n        A_stable = (v @ np.diag(w) @ np.linalg.inv(v)).real\n        return A_stable\n\n    return A\n</code></pre>"},{"location":"api/subspace/#llsi.subspace.SubspaceIdent.hankel","title":"<code>hankel(x, n)</code>  <code>staticmethod</code>","text":"<p>Construct Block Hankel matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Data array (N, n_channels).</p> required <code>n</code> <code>int</code> <p>Number of block rows.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Hankel matrix.</p> Source code in <code>src/llsi/subspace.py</code> <pre><code>@staticmethod\ndef hankel(x: np.ndarray, n: int) -&gt; np.ndarray:\n    \"\"\"\n    Construct Block Hankel matrix.\n\n    Args:\n        x: Data array (N, n_channels).\n        n: Number of block rows.\n\n    Returns:\n        np.ndarray: Hankel matrix.\n    \"\"\"\n    # x is (N, n_channels)\n    # We want a Hankel matrix with n block rows.\n    # If x has 1 column, it's standard Hankel.\n    # If x has m columns, it's block Hankel.\n\n    N_samples, n_channels = x.shape\n    # Number of block columns\n    # If we have n block rows, we need enough samples.\n    # H is (n * n_channels, N_cols)\n    # N_cols = N_samples - n + 1\n\n    # The original implementation:\n    # n = n // x.shape[1]  &lt;-- This suggests 'n' passed in is total rows, not block rows?\n    # \"n = order[0]; r = (2 * n + 1) * self.nu * self.ny\"\n    # \"Y = self.hankel(self.y, 2 * r)\"\n    # So 'n' passed to hankel is 2*r, which is total rows.\n\n    n_block_rows = n // n_channels\n    if n_block_rows &lt;= 0:\n        raise ValueError(f\"Not enough rows requested for Hankel matrix. n={n}, channels={n_channels}\")\n\n    N_cols = N_samples - n_block_rows + 1\n\n    if N_cols &lt;= 0:\n        raise ValueError(f\"Not enough samples for Hankel matrix. Samples={N_samples}, Block rows={n_block_rows}\")\n\n    # Efficient Hankel construction using stride_tricks or simple loop\n    # Original implementation used loops and lists.\n\n    # Let's stick to a cleaner loop or optimized approach.\n    # H = [x[0:N_cols], x[1:N_cols+1], ..., x[n_block_rows-1:]]\n    # But stacked vertically.\n\n    # Actually, let's follow the original logic to ensure compatibility, but clean it up.\n    # Original:\n    # for i in range(n): (where n is block rows)\n    #   for x_ in x.T: (iterate over channels)\n    #     A.append(x_[i : -n + i])\n\n    # Wait, x_[i : -n + i] ?\n    # if i=0, x_[0 : -n]. Length N-n.\n    # if i=n-1, x_[n-1 : -1]. Length N-n.\n    # This creates columns of length N-n.\n\n    A = []\n    for i in range(n_block_rows):\n        for j in range(n_channels):\n            # Column j of x\n            col = x[:, j]\n            # Slice\n            # We want N_cols elements starting at i.\n            # x[i : i + N_cols]\n            # N_cols = N_samples - n_block_rows + 1\n            # So x[i : i + N_samples - n_block_rows + 1]\n\n            # Original code: x_[i : -n + i]\n            # -n + i = -(n - i).\n            # If n=10, i=0 -&gt; :-10. Length N-10.\n            # If n=10, i=9 -&gt; 9:-1. Length N-10.\n            # So N_cols = N - n.\n            # This drops the last sample?\n            # Usually Hankel uses all samples.\n            # Let's use standard definition: N_cols = N - n + 1.\n\n            # But to match original behavior exactly if needed:\n            # The original code used `x_[i : -n + i]` which excludes the last `n-i` elements?\n            # No, `x_[:-n]` excludes last n elements.\n            # `x_[i : -n + i]` has length `(N - n + i) - i = N - n`.\n            # So it produces `N - n` columns.\n\n            # I will use `N - n_block_rows + 1` columns which is standard.\n            # But wait, if I change dimensions, it might break math downstream.\n            # Let's stick to `N - n_block_rows` columns as per original code implication (N-n).\n\n            end_idx = -n_block_rows + i\n            if end_idx == 0:\n                segment = col[i:]\n            else:\n                segment = col[i:end_idx]\n            A.append(segment)\n\n    return np.array(A)\n</code></pre>"},{"location":"api/subspace/#llsi.subspace.SubspaceIdent.lq","title":"<code>lq(A)</code>  <code>staticmethod</code>","text":"<p>Compute LQ decomposition.</p> Source code in <code>src/llsi/subspace.py</code> <pre><code>@staticmethod\ndef lq(A: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute LQ decomposition.\"\"\"\n    # LQ = (QR(A.T)).T = R.T Q.T\n    Q, R = scipy.linalg.qr(A.T, mode=\"economic\")\n    return R.T, Q.T\n</code></pre>"},{"location":"api/sysiddata/","title":"System Identification Data","text":""},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData","title":"<code>llsi.sysiddata.SysIdData</code>  <code>dataclass</code>","text":"<p>Container for time-series data used in system identification.</p> <p>Features: - Uses dataclasses for concise initialization - Supports equidistant and non-equidistant time sampling - Method chaining for fluent API design - Flexible data manipulation (crop, resample, filter, differentiate) - Pythonic interface (slicing, iteration, len)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create equidistant data\n&gt;&gt;&gt; data = SysIdData(Ts=0.01, u=u_array, y=y_array)\n&gt;&gt;&gt; # Slicing works like a list/array\n&gt;&gt;&gt; train_data = data[:1000]\n&gt;&gt;&gt; # Method chaining\n&gt;&gt;&gt; data.equidistant(N=500).lowpass(order=4, corner_frequency=10).plot()\n</code></pre> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>@dataclass\nclass SysIdData:\n    \"\"\"Container for time-series data used in system identification.\n\n    Features:\n    - Uses dataclasses for concise initialization\n    - Supports equidistant and non-equidistant time sampling\n    - Method chaining for fluent API design\n    - Flexible data manipulation (crop, resample, filter, differentiate)\n    - Pythonic interface (slicing, iteration, len)\n\n    Examples:\n        &gt;&gt;&gt; # Create equidistant data\n        &gt;&gt;&gt; data = SysIdData(Ts=0.01, u=u_array, y=y_array)\n        &gt;&gt;&gt; # Slicing works like a list/array\n        &gt;&gt;&gt; train_data = data[:1000]\n        &gt;&gt;&gt; # Method chaining\n        &gt;&gt;&gt; data.equidistant(N=500).lowpass(order=4, corner_frequency=10).plot()\n    \"\"\"\n\n    series: Dict[str, np.ndarray] = field(default_factory=dict)\n    t: Optional[np.ndarray] = None\n    Ts: Optional[float] = None\n    t_start: float = 0.0\n    means: Dict[str, float] = field(default_factory=dict)\n    stds: Dict[str, float] = field(default_factory=dict)\n\n    def __init__(\n        self, t: Optional[np.ndarray] = None, Ts: Optional[float] = None, t_start: Optional[float] = None, **kwargs: Any\n    ):\n        # Compatibility constructor: accept either `series` dict or individual series kwargs\n        series_arg = kwargs.pop(\"series\", None)\n        if series_arg is not None:\n            if not isinstance(series_arg, dict):\n                raise TypeError(\"series must be a dict of name: array\")\n            self.series = {k: np.asarray(v).ravel() for k, v in series_arg.items()}\n        else:\n            # remaining kwargs are interpreted as series\n            self.series = {k: np.asarray(v).ravel() for k, v in kwargs.items()}\n\n        # coerce time to numpy array if provided\n        self.t = np.asarray(t) if t is not None else None\n        self.Ts = Ts\n        if t_start is not None:\n            self.t_start = t_start\n        else:\n            if self.t is not None and self.t.size &gt; 0:\n                self.t_start = float(self.t[0])\n\n        # Initialize scaling state fields\n        self.means = {}\n        self.stds = {}\n\n        # Call post-init validations and conversions\n        self.__post_init__()\n\n    def __post_init__(self) -&gt; None:\n        self.logger = logging.getLogger(__name__)\n\n        # Validate Ts\n        if self.Ts is not None and self.Ts &lt;= 0:\n            raise ValueError(f\"Sampling time Ts must be positive, got {self.Ts}\")\n\n        # Ensure series arrays are numpy arrays and set N\n        for k, v in self:\n            self.series[k] = np.asarray(v).ravel()\n        if self.Ts is None and self.t is None:\n            raise ValueError(\"Either 't' (time vector) or 'Ts' (sampling time) must be provided.\")\n\n    @property\n    def N(self) -&gt; int:\n        \"\"\"Number of samples inferred from stored series. Returns 0 if no series.\"\"\"\n        if not self.series:\n            return 0\n        return next(iter(self.series.values())).shape[0]\n\n    @property\n    def time(self) -&gt; np.ndarray:\n        \"\"\"Time vector property. If `t` is None, construct from `t_start` and `Ts`.\"\"\"\n        if self.t is not None:\n            return np.asarray(self.t)\n        if self.N == 0:\n            return np.array([])\n        return self.t_start + np.arange(self.N) * self.Ts\n\n    def __getitem__(self, key: Union[str, slice, int]) -&gt; Union[np.ndarray, \"SysIdData\"]:\n        \"\"\"\n        Get a series by name OR slice the dataset by index.\n\n        Examples:\n            data[\"u\"]      -&gt; Returns numpy array of series 'u'\n            data[0:100]    -&gt; Returns a NEW SysIdData object cropped to first 100 samples\n            data[:50]      -&gt; First 50 samples\n        \"\"\"\n        if isinstance(key, str):\n            return self.series[key]\n        elif isinstance(key, (slice, int)):\n            # Handle slicing logic. If int, we treat it as a single-point slice or fail?\n            # Usually for time series objects, int access isn't well defined (row vs col).\n            # We treat int as a slice of length 1 for consistency, or let crop handle it.\n            start = key.start if isinstance(key, slice) else key\n            stop = key.stop if isinstance(key, slice) else key + 1\n            step = key.step if isinstance(key, slice) else 1\n\n            if step is not None and step != 1:\n                # Use downsample logic if step &gt; 1? For now, standard slicing.\n                # Standard array slicing supports steps, so we can support it manually\n                # but crop() is range based. Let's use crop for contiguous slices.\n                pass\n\n            # If it's a simple contiguous slice, use crop (safer for metadata)\n            if step is None or step == 1:\n                return self.crop(start=start, end=stop, inplace=False)\n            else:\n                # Complex slicing (e.g. ::2)\n                # We can implement this via direct array slicing\n                target = copy.deepcopy(self)\n                if target.t is not None:\n                    target.t = target.t[key]\n                    if len(target.t) &gt; 0:\n                        target.t_start = float(target.t[0])\n                else:\n                    # Update Ts and start if stepping\n                    if start:\n                        target.t_start += target.Ts * start\n                    target.Ts *= step\n\n                for k, v in target:\n                    target.series[k] = v[key]\n                return target\n\n        else:\n            raise TypeError(f\"Invalid index type: {type(key)}\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation for Jupyter notebooks and REPL.\"\"\"\n        series_names = list(self.series.keys())\n        time_info = \"\"\n        if self.N &gt; 0:\n            time_end = self.time[-1]\n            time_info = f\"  Time: {self.t_start:.2f} to {time_end:.2f}s\"\n        else:\n            time_info = \"  Time: empty\"\n\n        if self.Ts is not None:\n            header = f\"SysIdData(N={self.N}, Ts={self.Ts:.4f}s)\"\n        else:\n            header = f\"SysIdData(N={self.N}, Non-equidistant)\"\n\n        series_info = f\"  Series: {', '.join(series_names)}\" if series_names else \"  Series: (none)\"\n\n        return \"\\n\".join([header, time_info, series_info])\n\n    def copy(self) -&gt; \"SysIdData\":\n        \"\"\"Return a deep copy of this SysIdData object.\"\"\"\n        return copy.deepcopy(self)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of samples.\"\"\"\n        return self.N\n\n    def __contains__(self, key: str) -&gt; bool:\n        \"\"\"Check if a series exists in the dataset.\"\"\"\n        return key in self.series\n\n    def __iter__(self):\n        \"\"\"Iterate over (key, array) tuples of the series.\"\"\"\n        return iter(self.series.items())\n\n    def add_series(self, **kwargs: Any) -&gt; \"SysIdData\":\n        \"\"\"\n        Add time series to the dataset.\n\n        Args:\n            **kwargs: Time series data as keyword arguments (name=data).\n        \"\"\"\n        for key, val in kwargs.items():\n            s = np.asarray(val).ravel()\n            if self.series and s.shape[0] != self.N:\n                raise ValueError(\n                    f\"Length of vector to add ({s.shape[0]}) does not match existing series length ({self.N})\"\n                )\n\n            # Warn on duplicate keys\n            if key in self.series:\n                self.logger.warning(f\"Series '{key}' already exists. Overwriting.\")\n\n            # Check for NaN (Warning only, do not block)\n            if len(s) &gt; 0 and np.isnan(s).any():\n                self.logger.warning(f\"Series '{key}' contains NaN values.\")\n\n            self.series[key] = np.asarray(s)\n        return self\n\n    def remove(self, key: str) -&gt; \"SysIdData\":\n        \"\"\"Remove a time series and return self for chaining.\"\"\"\n        del self.series[key]\n        return self\n\n    # `time` property implemented above\n\n    def equidistant(\n        self, N: Optional[int] = None, inplace: bool = True, method: Union[str, Dict[str, str]] = \"linear\"\n    ) -&gt; \"SysIdData\":\n        \"\"\"\n        Resample data to be equidistant.\n\n        Modifies the object in-place by default.\n\n        Args:\n            N: Number of points for the new grid. If None, keeps current N.\n            inplace: If True, modify in-place. If False, return a copy.\n            method: Interpolation method. Can be:\n                - str: Single method applied to all series (e.g., \"linear\", \"previous\", \"cubic\").\n                  \"previous\" is equivalent to Zero-Order Hold (ZOH), suitable for step inputs.\n                - dict: Mapping series name to method, e.g., {\"u\": \"previous\", \"y\": \"linear\"}.\n                  Uses default \"linear\" for unmapped series.\n\n        Returns:\n            SysIdData: The resampled object (self if inplace=True, copy if inplace=False).\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n\n        # Validate interpolation methods\n        valid_methods = {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"previous\", \"next\"}\n        if isinstance(method, str):\n            if method not in valid_methods:\n                raise ValueError(f\"Invalid interpolation method '{method}'. Must be one of {valid_methods}\")\n            method_dict = dict.fromkeys(target.series.keys(), method)\n        else:\n            method_dict = {}\n            for k in target.series.keys():\n                m = method.get(k, \"linear\")\n                if m not in valid_methods:\n                    raise ValueError(f\"Invalid interpolation method '{m}' for series '{k}'\")\n                method_dict[k] = m\n\n        if N is None:\n            N = target.N\n\n        if N &lt; target.N:\n            target.logger.warning(\"Downsampling without filter! Aliasing may occur.\")\n\n        t_current = np.asarray(target.time)\n        if t_current.size == 0:\n            return target\n\n        t_start = t_current[0]\n        t_end = t_current[-1]\n        t_new = np.linspace(t_start, t_end, N)\n\n        if target.series:\n            keys = list(target.series.keys())\n\n            # Resample each series with its specified method\n            new_matrix = np.empty((len(keys), N))\n            for i, k in enumerate(keys):\n                f = scipy.interpolate.interp1d(\n                    t_current, target.series[k], kind=method_dict[k], axis=0, fill_value=\"extrapolate\"\n                )\n                new_matrix[i, :] = f(t_new)\n\n            for i, k in enumerate(keys):\n                target.series[k] = new_matrix[i, :]\n\n        target.Ts = (t_end - t_start) / (N - 1) if N &gt; 1 else 0.0\n        target.t = None\n        return target\n\n    def center(self, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Remove the mean from all series and store the means for later unscaling.\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n\n        for k, v in target:\n            mu = np.mean(v)\n            target.series[k] = v - mu\n            # Store the mean (additively: if already centered, add to existing)\n            target.means[k] = target.means.get(k, 0.0) + mu\n\n        return target\n\n    def standardize(self, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Remove mean and scale to unit variance (Z-score normalization).\n\n        Stores both means and standard deviations for later unscaling.\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n\n        # First center (this fills target.means)\n        target.center(inplace=True)\n\n        # Then scale to unit variance\n        for k, v in target:\n            sigma = np.std(v)\n            if sigma &lt; 1e-12:  # Protect against division by zero for constant signals\n                sigma = 1.0\n                warnings.warn(f\"Series '{k}' is constant. Skipping scaling.\", stacklevel=2)\n\n            target.series[k] = v / sigma\n            # Store the scaling factor (multiplicatively)\n            target.stds[k] = target.stds.get(k, 1.0) * sigma\n\n        return target\n\n    def unscale(self, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Reverse center() and standardize() transformations (back to physical units).\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n\n        # 1. Reverse scaling (multiply by stored stds)\n        if target.stds:\n            for k, v in target:\n                if k in target.stds:\n                    target.series[k] = v * target.stds[k]\n            target.stds.clear()  # Reset\n\n        # 2. Reverse centering (add back stored means)\n        if target.means:\n            for k, v in target:\n                if k in target.means:\n                    target.series[k] = v + target.means[k]\n            target.means.clear()  # Reset\n\n        return target\n\n    def apply_scaling_from(self, source: \"SysIdData\", inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Apply the scaling (means/stds) from another dataset to this one.\n\n        Important for train/test splits: test data should be scaled using training statistics.\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n\n        # Apply means from source\n        for k, v in target:\n            if k in source.means:\n                mu = source.means[k]\n                target.series[k] = v - mu\n                target.means[k] = target.means.get(k, 0.0) + mu\n\n        # Apply stds from source\n        for k, _ in target:\n            if k in source.stds:\n                sigma = source.stds[k]\n                target.series[k] = target.series[k] / sigma\n                target.stds[k] = target.stds.get(k, 1.0) * sigma\n\n        return target\n\n    def crop(self, start: Optional[int] = None, end: Optional[int] = None, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Crop the data to a subset of samples.\n\n        Args:\n            start: Start index (default: 0).\n            end: End index (default: N, exclusive).\n            inplace: If True, modify in-place. If False, return a copy.\n\n        Returns:\n            SysIdData: The cropped object (self if inplace=True, copy if inplace=False).\n        \"\"\"\n        start = start or 0\n        end = end or self.N\n\n        target = self if inplace else copy.deepcopy(self)\n\n        # Update time vector\n        if target.t is not None:\n            # Non-equidistant: crop time vector and update t_start\n            target.t = target.t[start:end]\n            if len(target.t) &gt; 0:\n                target.t_start = float(target.t[0])\n        else:\n            # Equidistant: update t_start based on the number of skipped samples\n            target.t_start += target.Ts * start\n\n        # Crop all series\n        for k, v in target:\n            target.series[k] = v[start:end]\n\n        return target\n\n    def split(\n        self, proportion: Optional[float] = None, sample: Optional[int] = None\n    ) -&gt; Tuple[\"SysIdData\", \"SysIdData\"]:\n        \"\"\"\n        Split the data into two sets.\n\n        Args:\n            proportion: Ratio of the first set (0 to 1). Default 0.5.\n            sample: Index to split at. Overrides proportion.\n\n        Returns:\n            A tuple containing two SysIdData objects.\n        \"\"\"\n        if not sample and not proportion:\n            proportion = 0.5\n\n        if sample is None and proportion is not None:\n            sample = int(round(self.N * proportion))\n\n        # print(f\"Splitting at {sample}\")\n\n        d1 = copy.deepcopy(self)\n        d1.crop(end=sample, inplace=True)\n        d2 = copy.deepcopy(self)\n        d2.crop(start=sample, inplace=True)\n\n        return d1, d2\n\n    def resample(self, factor: float, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Resample the data using Fourier method (preserves frequency content).\n\n        Args:\n            factor: Resampling factor. &gt;1 upsamples, &lt;1 downsamples.\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n        N_new = int(target.N * factor)\n        for k, v in target:\n            target.series[k] = scipy.signal.resample(v, N_new)\n\n        if target.t is not None:\n            target.t = scipy.signal.resample(target.t, N_new)\n        else:\n            target.Ts = target.Ts / factor\n\n        return target\n\n    def downsample(self, q: int, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Downsample the data by an integer factor.\n\n        Applies an anti-aliasing filter (Chebyshev type I) before downsampling.\n\n        Args:\n            q: Downsampling factor.\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n        if q &lt; 1:\n            raise ValueError(f\"Downsampling factor must be &gt;= 1, got {q}\")\n\n        for k, v in target:\n            target.series[k] = scipy.signal.decimate(v, q)\n\n        if target.Ts is not None:\n            target.Ts *= q\n        if target.t is not None:\n            target.t = target.t[::q]\n\n        return target\n\n    def lowpass(self, order: int, corner_frequency: float, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Apply a Butterworth lowpass filter to all data series.\n\n        Args:\n            order: The order of the filter.\n            corner_frequency: The corner frequency in Hz. Must be less than the Nyquist frequency.\n            inplace: If True, modify in-place. If False, return a copy.\n\n        Returns:\n            SysIdData: The filtered object (self if inplace=True, copy if inplace=False).\n\n        Raises:\n            ValueError: If Ts is not defined or corner_frequency &gt;= Nyquist frequency.\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n        if target.Ts is None:\n            raise ValueError(\"Sampling time 'Ts' is required for filtering.\")\n\n        # Calculate Nyquist frequency and validate corner_frequency\n        nyquist = 1.0 / (2 * target.Ts)\n        if corner_frequency &gt;= nyquist:\n            raise ValueError(\n                f\"Corner frequency ({corner_frequency:.2f} Hz) must be less than Nyquist frequency \"\n                f\"({nyquist:.2f} Hz). Increase sampling rate (decrease Ts={target.Ts}) or use a lower corner frequency. \"\n                f\"Required Ts &lt; {1.0 / (2 * corner_frequency):.4f}s\"\n            )\n\n        sos = scipy.signal.butter(order, corner_frequency, \"low\", analog=False, fs=1.0 / target.Ts, output=\"sos\")\n        for k, _ in target:\n            target.series[k] = scipy.signal.sosfilt(sos, target.series[k])\n        return target\n\n    def differentiate(self, key: str, new_key: Optional[str] = None, inplace: bool = True) -&gt; \"SysIdData\":\n        \"\"\"\n        Compute time derivative of a series using central differences.\n\n        Uses numpy.gradient with central differences (edge_order=2) which:\n        - Preserves array length (unlike np.diff which shortens by 1)\n        - Uses second-order accurate central differences for interior points\n        - Uses second-order forward/backward differences at boundaries\n\n        Args:\n            key: Name of the series to differentiate (e.g., \"y\" for position).\n            new_key: Name for the derivative series. Defaults to \"d{key}\" (e.g., \"dy\" for \"y\").\n            inplace: If True, modify in-place. If False, return a copy.\n\n        Returns:\n            SysIdData: The object with the derivative added (self if inplace=True, copy if inplace=False).\n\n        Example:\n            &gt;&gt;&gt; data = SysIdData(Ts=0.01, position=np.array([0, 1, 2, 3, 4]))\n            &gt;&gt;&gt; data.differentiate(\"position\", \"velocity\", inplace=True)\n            &gt;&gt;&gt; # Now data has \"position\" and \"velocity\" series\n        \"\"\"\n        target = self if inplace else copy.deepcopy(self)\n\n        if key not in target.series:\n            raise ValueError(f\"Series '{key}' not found in dataset.\")\n\n        # Determine the spacing for gradient calculation\n        # For equidistant data, use Ts; for non-equidistant, use the time vector\n        if target.Ts is not None:\n            dt_arg = target.Ts\n        else:\n            dt_arg = target.time\n\n        # Compute derivative using central differences\n        y_dot = np.gradient(target.series[key], dt_arg, edge_order=2)\n\n        # Determine output key name\n        dest_key = new_key if new_key else f\"d{key}\"\n        target.series[dest_key] = y_dot\n\n        return target\n\n    def plot(self) -&gt; None:\n        \"\"\"Plot the data.\"\"\"\n        try:\n            import matplotlib.pyplot as plt\n        except ImportError:\n            raise ImportError(\n                \"matplotlib is required for plotting. Install it with 'pip install llsi[plot]'.\"\n            ) from None\n\n        t = self.time\n        n_series = len(self.series)\n\n        fig, axes = plt.subplots(n_series, 1, sharex=True, figsize=(10, 2 * n_series))\n        if n_series == 1:\n            axes = [axes]\n\n        for ax, (key, val) in zip(axes, self):\n            ax.plot(t, val, label=key)\n            ax.legend()\n            ax.grid(True)\n\n        plt.xlabel(\"Time\")\n        plt.show()\n\n    @staticmethod\n    def generate_prbs(N: int, Ts: float, seed: int = 42) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Generate a Pseudo-Random Binary Sequence (PRBS).\n\n        Args:\n            N: Number of samples.\n            Ts: Sampling time.\n            seed: Random seed.\n\n        Returns:\n            Tuple of (time vector, PRBS signal).\n        \"\"\"\n        t = np.linspace(0, Ts * N, num=N, endpoint=False)\n        u = _math.generate_prbs_sequence(N, seed)\n        return t, u\n\n    def to_pandas(self) -&gt; Any:\n        \"\"\"\n        Convert to pandas DataFrame.\n\n        Returns:\n            pandas.DataFrame: The data as a DataFrame.\n        \"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"pandas is required for this method. Install it with 'pip install llsi[data]'.\") from None\n\n        df = pd.DataFrame(self.series)\n        df.index = self.time\n        return df\n\n    @classmethod\n    def from_pandas(cls, df, time_col=None, Ts=None):\n        \"\"\"\n        Create SysIdData from pandas DataFrame.\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            The dataframe containing the data.\n        time_col : str, optional\n            Name of the column to use as time. If None, the index is used.\n        Ts : float, optional\n            Sampling time. If None, it is inferred from the time index if possible.\n        \"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"pandas is required for this method. Install it with 'pip install llsi[data]'.\") from None\n\n        if time_col:\n            t_values = df[time_col].values\n            data_df = df.drop(columns=[time_col])\n        else:\n            t_values = df.index.values\n            data_df = df\n\n        series_data = {col: data_df[col].values for col in data_df.columns}\n\n        # Infer Ts if not provided\n        t_start = None\n        t_vec = None\n\n        if Ts is None:\n            # Check if t_values are numeric or datetime\n            if pd.api.types.is_numeric_dtype(t_values):\n                diffs = np.diff(t_values)\n                if len(diffs) &gt; 0 and np.allclose(diffs, diffs[0]):\n                    Ts = float(diffs[0])\n                    t_start = float(t_values[0])\n                else:\n                    t_vec = t_values\n            elif pd.api.types.is_datetime64_any_dtype(t_values):\n                # Convert to seconds relative to start\n                t_start_timestamp = t_values[0]\n                t_seconds = (t_values - t_start_timestamp) / np.timedelta64(1, \"s\")\n\n                diffs = np.diff(t_seconds)\n                if len(diffs) &gt; 0 and np.allclose(diffs, diffs[0]):\n                    Ts = float(diffs[0])\n                    t_start = 0.0  # Relative time\n                else:\n                    t_vec = t_seconds\n                    t_start = 0.0\n            else:\n                # Fallback, maybe just index\n                t_vec = np.arange(len(t_values))\n                Ts = 1.0\n                t_start = 0.0\n        else:\n            # Ts provided\n            if pd.api.types.is_numeric_dtype(t_values):\n                t_start = float(t_values[0])\n            else:\n                t_start = 0.0\n\n        return cls(t=t_vec, Ts=Ts, t_start=t_start, **series_data)\n\n    @classmethod\n    def from_logfile(\n        cls,\n        path,\n        time_col=\"datetime\",\n        value_col=\"temperature\",\n        pivot_col=\"property_name\",\n        datetime_format=None,\n        sep=\",\",\n        **kwargs,\n    ):\n        \"\"\"\n        Loads a logfile, pivots it, and automatically regularizes the time grid using the\n        internal equidistant() method.\n\n        Parameters\n        ----------\n        path : str\n            Path to the CSV logfile.\n        time_col : str\n            Column name containing datetime information.\n        value_col : str\n            Column name containing the measured value.\n        pivot_col : str\n            Column name that defines different signals.\n        datetime_format : str\n            Format string for faster datetime parsing.\n        sep : str\n            CSV separator.\n        **kwargs\n            Additional arguments passed to pd.read_csv.\n        \"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"pandas is required for this method.\") from None\n\n        # 1. Load Raw Data\n        df = pd.read_csv(path, sep=sep, **kwargs)\n\n        if time_col not in df.columns:\n            raise KeyError(f\"Time column '{time_col}' not found.\")\n\n        # Convert to datetime\n        df[time_col] = pd.to_datetime(df[time_col], format=datetime_format)\n\n        # 2. Pivot (Make wide)\n        # We use pivot_table with 'first' to handle duplicates strictly,\n        # or just pivot if we are sure data is unique per timestamp.\n        # pivot_table is safer for dirty logs.\n        if pivot_col:\n            df_wide = df.pivot_table(index=time_col, columns=pivot_col, values=value_col, aggfunc=\"first\")\n        else:\n            # Assume it is already wide, just set index\n            df_wide = df.set_index(time_col)\n\n        # Handle NaNs from pivoting (async sensors):\n        # We fill forward/backward just to get continuous arrays for the raw object.\n        # Real resampling happens in equidistant() later.\n        df_wide = df_wide.ffill().bfill()\n\n        if df_wide.empty:\n            raise ValueError(\"Dataframe is empty after loading and pivoting.\")\n\n        # 3. Calculate Relative Time Vector\n        t_abs = df_wide.index\n        # Convert to seconds starting at 0\n        t_sec = (t_abs - t_abs[0]).total_seconds().values\n\n        # 4. Infer Sampling Time (Ts) and N from the raw data\n        # Using median is robust against missing samples or small jitter\n        dt_raw = np.diff(t_sec)\n        if len(dt_raw) &gt; 0:\n            Ts_est = float(np.median(dt_raw))\n        else:\n            Ts_est = 1.0  # Fallback for single point\n\n        # Calculate logical N based on duration and estimated Ts\n        duration = t_sec[-1]\n        if Ts_est &gt; 0:\n            N_est = int(np.round(duration / Ts_est)) + 1\n        else:\n            N_est = len(t_sec)\n\n        # 5. Create Raw SysIdData Object\n        series_data = {col: df_wide[col].values for col in df_wide.columns}\n\n        # Initialize with the uneven raw time vector\n        raw_obj = cls(t=t_sec, Ts=None, **series_data)\n\n        # 6. Apply internal resampling to force equidistant grid\n        # This uses the class's own interpolation logic\n        return raw_obj.equidistant(N=N_est)\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.N","title":"<code>N</code>  <code>property</code>","text":"<p>Number of samples inferred from stored series. Returns 0 if no series.</p>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.time","title":"<code>time</code>  <code>property</code>","text":"<p>Time vector property. If <code>t</code> is None, construct from <code>t_start</code> and <code>Ts</code>.</p>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.__contains__","title":"<code>__contains__(key)</code>","text":"<p>Check if a series exists in the dataset.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def __contains__(self, key: str) -&gt; bool:\n    \"\"\"Check if a series exists in the dataset.\"\"\"\n    return key in self.series\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get a series by name OR slice the dataset by index.</p> <p>Examples:</p> <p>data[\"u\"]      -&gt; Returns numpy array of series 'u' data[0:100]    -&gt; Returns a NEW SysIdData object cropped to first 100 samples data[:50]      -&gt; First 50 samples</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def __getitem__(self, key: Union[str, slice, int]) -&gt; Union[np.ndarray, \"SysIdData\"]:\n    \"\"\"\n    Get a series by name OR slice the dataset by index.\n\n    Examples:\n        data[\"u\"]      -&gt; Returns numpy array of series 'u'\n        data[0:100]    -&gt; Returns a NEW SysIdData object cropped to first 100 samples\n        data[:50]      -&gt; First 50 samples\n    \"\"\"\n    if isinstance(key, str):\n        return self.series[key]\n    elif isinstance(key, (slice, int)):\n        # Handle slicing logic. If int, we treat it as a single-point slice or fail?\n        # Usually for time series objects, int access isn't well defined (row vs col).\n        # We treat int as a slice of length 1 for consistency, or let crop handle it.\n        start = key.start if isinstance(key, slice) else key\n        stop = key.stop if isinstance(key, slice) else key + 1\n        step = key.step if isinstance(key, slice) else 1\n\n        if step is not None and step != 1:\n            # Use downsample logic if step &gt; 1? For now, standard slicing.\n            # Standard array slicing supports steps, so we can support it manually\n            # but crop() is range based. Let's use crop for contiguous slices.\n            pass\n\n        # If it's a simple contiguous slice, use crop (safer for metadata)\n        if step is None or step == 1:\n            return self.crop(start=start, end=stop, inplace=False)\n        else:\n            # Complex slicing (e.g. ::2)\n            # We can implement this via direct array slicing\n            target = copy.deepcopy(self)\n            if target.t is not None:\n                target.t = target.t[key]\n                if len(target.t) &gt; 0:\n                    target.t_start = float(target.t[0])\n            else:\n                # Update Ts and start if stepping\n                if start:\n                    target.t_start += target.Ts * start\n                target.Ts *= step\n\n            for k, v in target:\n                target.series[k] = v[key]\n            return target\n\n    else:\n        raise TypeError(f\"Invalid index type: {type(key)}\")\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over (key, array) tuples of the series.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over (key, array) tuples of the series.\"\"\"\n    return iter(self.series.items())\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.__len__","title":"<code>__len__()</code>","text":"<p>Return number of samples.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of samples.\"\"\"\n    return self.N\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation for Jupyter notebooks and REPL.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation for Jupyter notebooks and REPL.\"\"\"\n    series_names = list(self.series.keys())\n    time_info = \"\"\n    if self.N &gt; 0:\n        time_end = self.time[-1]\n        time_info = f\"  Time: {self.t_start:.2f} to {time_end:.2f}s\"\n    else:\n        time_info = \"  Time: empty\"\n\n    if self.Ts is not None:\n        header = f\"SysIdData(N={self.N}, Ts={self.Ts:.4f}s)\"\n    else:\n        header = f\"SysIdData(N={self.N}, Non-equidistant)\"\n\n    series_info = f\"  Series: {', '.join(series_names)}\" if series_names else \"  Series: (none)\"\n\n    return \"\\n\".join([header, time_info, series_info])\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.add_series","title":"<code>add_series(**kwargs)</code>","text":"<p>Add time series to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Time series data as keyword arguments (name=data).</p> <code>{}</code> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def add_series(self, **kwargs: Any) -&gt; \"SysIdData\":\n    \"\"\"\n    Add time series to the dataset.\n\n    Args:\n        **kwargs: Time series data as keyword arguments (name=data).\n    \"\"\"\n    for key, val in kwargs.items():\n        s = np.asarray(val).ravel()\n        if self.series and s.shape[0] != self.N:\n            raise ValueError(\n                f\"Length of vector to add ({s.shape[0]}) does not match existing series length ({self.N})\"\n            )\n\n        # Warn on duplicate keys\n        if key in self.series:\n            self.logger.warning(f\"Series '{key}' already exists. Overwriting.\")\n\n        # Check for NaN (Warning only, do not block)\n        if len(s) &gt; 0 and np.isnan(s).any():\n            self.logger.warning(f\"Series '{key}' contains NaN values.\")\n\n        self.series[key] = np.asarray(s)\n    return self\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.apply_scaling_from","title":"<code>apply_scaling_from(source, inplace=True)</code>","text":"<p>Apply the scaling (means/stds) from another dataset to this one.</p> <p>Important for train/test splits: test data should be scaled using training statistics.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def apply_scaling_from(self, source: \"SysIdData\", inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Apply the scaling (means/stds) from another dataset to this one.\n\n    Important for train/test splits: test data should be scaled using training statistics.\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n\n    # Apply means from source\n    for k, v in target:\n        if k in source.means:\n            mu = source.means[k]\n            target.series[k] = v - mu\n            target.means[k] = target.means.get(k, 0.0) + mu\n\n    # Apply stds from source\n    for k, _ in target:\n        if k in source.stds:\n            sigma = source.stds[k]\n            target.series[k] = target.series[k] / sigma\n            target.stds[k] = target.stds.get(k, 1.0) * sigma\n\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.center","title":"<code>center(inplace=True)</code>","text":"<p>Remove the mean from all series and store the means for later unscaling.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def center(self, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Remove the mean from all series and store the means for later unscaling.\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n\n    for k, v in target:\n        mu = np.mean(v)\n        target.series[k] = v - mu\n        # Store the mean (additively: if already centered, add to existing)\n        target.means[k] = target.means.get(k, 0.0) + mu\n\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.copy","title":"<code>copy()</code>","text":"<p>Return a deep copy of this SysIdData object.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def copy(self) -&gt; \"SysIdData\":\n    \"\"\"Return a deep copy of this SysIdData object.\"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.crop","title":"<code>crop(start=None, end=None, inplace=True)</code>","text":"<p>Crop the data to a subset of samples.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[int]</code> <p>Start index (default: 0).</p> <code>None</code> <code>end</code> <code>Optional[int]</code> <p>End index (default: N, exclusive).</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True, modify in-place. If False, return a copy.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SysIdData</code> <code>SysIdData</code> <p>The cropped object (self if inplace=True, copy if inplace=False).</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def crop(self, start: Optional[int] = None, end: Optional[int] = None, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Crop the data to a subset of samples.\n\n    Args:\n        start: Start index (default: 0).\n        end: End index (default: N, exclusive).\n        inplace: If True, modify in-place. If False, return a copy.\n\n    Returns:\n        SysIdData: The cropped object (self if inplace=True, copy if inplace=False).\n    \"\"\"\n    start = start or 0\n    end = end or self.N\n\n    target = self if inplace else copy.deepcopy(self)\n\n    # Update time vector\n    if target.t is not None:\n        # Non-equidistant: crop time vector and update t_start\n        target.t = target.t[start:end]\n        if len(target.t) &gt; 0:\n            target.t_start = float(target.t[0])\n    else:\n        # Equidistant: update t_start based on the number of skipped samples\n        target.t_start += target.Ts * start\n\n    # Crop all series\n    for k, v in target:\n        target.series[k] = v[start:end]\n\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.differentiate","title":"<code>differentiate(key, new_key=None, inplace=True)</code>","text":"<p>Compute time derivative of a series using central differences.</p> <p>Uses numpy.gradient with central differences (edge_order=2) which: - Preserves array length (unlike np.diff which shortens by 1) - Uses second-order accurate central differences for interior points - Uses second-order forward/backward differences at boundaries</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the series to differentiate (e.g., \"y\" for position).</p> required <code>new_key</code> <code>Optional[str]</code> <p>Name for the derivative series. Defaults to \"d{key}\" (e.g., \"dy\" for \"y\").</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True, modify in-place. If False, return a copy.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SysIdData</code> <code>SysIdData</code> <p>The object with the derivative added (self if inplace=True, copy if inplace=False).</p> Example <p>data = SysIdData(Ts=0.01, position=np.array([0, 1, 2, 3, 4])) data.differentiate(\"position\", \"velocity\", inplace=True)</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def differentiate(self, key: str, new_key: Optional[str] = None, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Compute time derivative of a series using central differences.\n\n    Uses numpy.gradient with central differences (edge_order=2) which:\n    - Preserves array length (unlike np.diff which shortens by 1)\n    - Uses second-order accurate central differences for interior points\n    - Uses second-order forward/backward differences at boundaries\n\n    Args:\n        key: Name of the series to differentiate (e.g., \"y\" for position).\n        new_key: Name for the derivative series. Defaults to \"d{key}\" (e.g., \"dy\" for \"y\").\n        inplace: If True, modify in-place. If False, return a copy.\n\n    Returns:\n        SysIdData: The object with the derivative added (self if inplace=True, copy if inplace=False).\n\n    Example:\n        &gt;&gt;&gt; data = SysIdData(Ts=0.01, position=np.array([0, 1, 2, 3, 4]))\n        &gt;&gt;&gt; data.differentiate(\"position\", \"velocity\", inplace=True)\n        &gt;&gt;&gt; # Now data has \"position\" and \"velocity\" series\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n\n    if key not in target.series:\n        raise ValueError(f\"Series '{key}' not found in dataset.\")\n\n    # Determine the spacing for gradient calculation\n    # For equidistant data, use Ts; for non-equidistant, use the time vector\n    if target.Ts is not None:\n        dt_arg = target.Ts\n    else:\n        dt_arg = target.time\n\n    # Compute derivative using central differences\n    y_dot = np.gradient(target.series[key], dt_arg, edge_order=2)\n\n    # Determine output key name\n    dest_key = new_key if new_key else f\"d{key}\"\n    target.series[dest_key] = y_dot\n\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.differentiate--now-data-has-position-and-velocity-series","title":"Now data has \"position\" and \"velocity\" series","text":""},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.downsample","title":"<code>downsample(q, inplace=True)</code>","text":"<p>Downsample the data by an integer factor.</p> <p>Applies an anti-aliasing filter (Chebyshev type I) before downsampling.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>int</code> <p>Downsampling factor.</p> required Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def downsample(self, q: int, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Downsample the data by an integer factor.\n\n    Applies an anti-aliasing filter (Chebyshev type I) before downsampling.\n\n    Args:\n        q: Downsampling factor.\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n    if q &lt; 1:\n        raise ValueError(f\"Downsampling factor must be &gt;= 1, got {q}\")\n\n    for k, v in target:\n        target.series[k] = scipy.signal.decimate(v, q)\n\n    if target.Ts is not None:\n        target.Ts *= q\n    if target.t is not None:\n        target.t = target.t[::q]\n\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.equidistant","title":"<code>equidistant(N=None, inplace=True, method='linear')</code>","text":"<p>Resample data to be equidistant.</p> <p>Modifies the object in-place by default.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Optional[int]</code> <p>Number of points for the new grid. If None, keeps current N.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True, modify in-place. If False, return a copy.</p> <code>True</code> <code>method</code> <code>Union[str, Dict[str, str]]</code> <p>Interpolation method. Can be: - str: Single method applied to all series (e.g., \"linear\", \"previous\", \"cubic\").   \"previous\" is equivalent to Zero-Order Hold (ZOH), suitable for step inputs. - dict: Mapping series name to method, e.g., {\"u\": \"previous\", \"y\": \"linear\"}.   Uses default \"linear\" for unmapped series.</p> <code>'linear'</code> <p>Returns:</p> Name Type Description <code>SysIdData</code> <code>SysIdData</code> <p>The resampled object (self if inplace=True, copy if inplace=False).</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def equidistant(\n    self, N: Optional[int] = None, inplace: bool = True, method: Union[str, Dict[str, str]] = \"linear\"\n) -&gt; \"SysIdData\":\n    \"\"\"\n    Resample data to be equidistant.\n\n    Modifies the object in-place by default.\n\n    Args:\n        N: Number of points for the new grid. If None, keeps current N.\n        inplace: If True, modify in-place. If False, return a copy.\n        method: Interpolation method. Can be:\n            - str: Single method applied to all series (e.g., \"linear\", \"previous\", \"cubic\").\n              \"previous\" is equivalent to Zero-Order Hold (ZOH), suitable for step inputs.\n            - dict: Mapping series name to method, e.g., {\"u\": \"previous\", \"y\": \"linear\"}.\n              Uses default \"linear\" for unmapped series.\n\n    Returns:\n        SysIdData: The resampled object (self if inplace=True, copy if inplace=False).\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n\n    # Validate interpolation methods\n    valid_methods = {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"previous\", \"next\"}\n    if isinstance(method, str):\n        if method not in valid_methods:\n            raise ValueError(f\"Invalid interpolation method '{method}'. Must be one of {valid_methods}\")\n        method_dict = dict.fromkeys(target.series.keys(), method)\n    else:\n        method_dict = {}\n        for k in target.series.keys():\n            m = method.get(k, \"linear\")\n            if m not in valid_methods:\n                raise ValueError(f\"Invalid interpolation method '{m}' for series '{k}'\")\n            method_dict[k] = m\n\n    if N is None:\n        N = target.N\n\n    if N &lt; target.N:\n        target.logger.warning(\"Downsampling without filter! Aliasing may occur.\")\n\n    t_current = np.asarray(target.time)\n    if t_current.size == 0:\n        return target\n\n    t_start = t_current[0]\n    t_end = t_current[-1]\n    t_new = np.linspace(t_start, t_end, N)\n\n    if target.series:\n        keys = list(target.series.keys())\n\n        # Resample each series with its specified method\n        new_matrix = np.empty((len(keys), N))\n        for i, k in enumerate(keys):\n            f = scipy.interpolate.interp1d(\n                t_current, target.series[k], kind=method_dict[k], axis=0, fill_value=\"extrapolate\"\n            )\n            new_matrix[i, :] = f(t_new)\n\n        for i, k in enumerate(keys):\n            target.series[k] = new_matrix[i, :]\n\n    target.Ts = (t_end - t_start) / (N - 1) if N &gt; 1 else 0.0\n    target.t = None\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.from_logfile","title":"<code>from_logfile(path, time_col='datetime', value_col='temperature', pivot_col='property_name', datetime_format=None, sep=',', **kwargs)</code>  <code>classmethod</code>","text":"<p>Loads a logfile, pivots it, and automatically regularizes the time grid using the internal equidistant() method.</p>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.from_logfile--parameters","title":"Parameters","text":"<p>path : str     Path to the CSV logfile. time_col : str     Column name containing datetime information. value_col : str     Column name containing the measured value. pivot_col : str     Column name that defines different signals. datetime_format : str     Format string for faster datetime parsing. sep : str     CSV separator. **kwargs     Additional arguments passed to pd.read_csv.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>@classmethod\ndef from_logfile(\n    cls,\n    path,\n    time_col=\"datetime\",\n    value_col=\"temperature\",\n    pivot_col=\"property_name\",\n    datetime_format=None,\n    sep=\",\",\n    **kwargs,\n):\n    \"\"\"\n    Loads a logfile, pivots it, and automatically regularizes the time grid using the\n    internal equidistant() method.\n\n    Parameters\n    ----------\n    path : str\n        Path to the CSV logfile.\n    time_col : str\n        Column name containing datetime information.\n    value_col : str\n        Column name containing the measured value.\n    pivot_col : str\n        Column name that defines different signals.\n    datetime_format : str\n        Format string for faster datetime parsing.\n    sep : str\n        CSV separator.\n    **kwargs\n        Additional arguments passed to pd.read_csv.\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for this method.\") from None\n\n    # 1. Load Raw Data\n    df = pd.read_csv(path, sep=sep, **kwargs)\n\n    if time_col not in df.columns:\n        raise KeyError(f\"Time column '{time_col}' not found.\")\n\n    # Convert to datetime\n    df[time_col] = pd.to_datetime(df[time_col], format=datetime_format)\n\n    # 2. Pivot (Make wide)\n    # We use pivot_table with 'first' to handle duplicates strictly,\n    # or just pivot if we are sure data is unique per timestamp.\n    # pivot_table is safer for dirty logs.\n    if pivot_col:\n        df_wide = df.pivot_table(index=time_col, columns=pivot_col, values=value_col, aggfunc=\"first\")\n    else:\n        # Assume it is already wide, just set index\n        df_wide = df.set_index(time_col)\n\n    # Handle NaNs from pivoting (async sensors):\n    # We fill forward/backward just to get continuous arrays for the raw object.\n    # Real resampling happens in equidistant() later.\n    df_wide = df_wide.ffill().bfill()\n\n    if df_wide.empty:\n        raise ValueError(\"Dataframe is empty after loading and pivoting.\")\n\n    # 3. Calculate Relative Time Vector\n    t_abs = df_wide.index\n    # Convert to seconds starting at 0\n    t_sec = (t_abs - t_abs[0]).total_seconds().values\n\n    # 4. Infer Sampling Time (Ts) and N from the raw data\n    # Using median is robust against missing samples or small jitter\n    dt_raw = np.diff(t_sec)\n    if len(dt_raw) &gt; 0:\n        Ts_est = float(np.median(dt_raw))\n    else:\n        Ts_est = 1.0  # Fallback for single point\n\n    # Calculate logical N based on duration and estimated Ts\n    duration = t_sec[-1]\n    if Ts_est &gt; 0:\n        N_est = int(np.round(duration / Ts_est)) + 1\n    else:\n        N_est = len(t_sec)\n\n    # 5. Create Raw SysIdData Object\n    series_data = {col: df_wide[col].values for col in df_wide.columns}\n\n    # Initialize with the uneven raw time vector\n    raw_obj = cls(t=t_sec, Ts=None, **series_data)\n\n    # 6. Apply internal resampling to force equidistant grid\n    # This uses the class's own interpolation logic\n    return raw_obj.equidistant(N=N_est)\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.from_pandas","title":"<code>from_pandas(df, time_col=None, Ts=None)</code>  <code>classmethod</code>","text":"<p>Create SysIdData from pandas DataFrame.</p>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.from_pandas--parameters","title":"Parameters","text":"<p>df : pandas.DataFrame     The dataframe containing the data. time_col : str, optional     Name of the column to use as time. If None, the index is used. Ts : float, optional     Sampling time. If None, it is inferred from the time index if possible.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>@classmethod\ndef from_pandas(cls, df, time_col=None, Ts=None):\n    \"\"\"\n    Create SysIdData from pandas DataFrame.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the data.\n    time_col : str, optional\n        Name of the column to use as time. If None, the index is used.\n    Ts : float, optional\n        Sampling time. If None, it is inferred from the time index if possible.\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for this method. Install it with 'pip install llsi[data]'.\") from None\n\n    if time_col:\n        t_values = df[time_col].values\n        data_df = df.drop(columns=[time_col])\n    else:\n        t_values = df.index.values\n        data_df = df\n\n    series_data = {col: data_df[col].values for col in data_df.columns}\n\n    # Infer Ts if not provided\n    t_start = None\n    t_vec = None\n\n    if Ts is None:\n        # Check if t_values are numeric or datetime\n        if pd.api.types.is_numeric_dtype(t_values):\n            diffs = np.diff(t_values)\n            if len(diffs) &gt; 0 and np.allclose(diffs, diffs[0]):\n                Ts = float(diffs[0])\n                t_start = float(t_values[0])\n            else:\n                t_vec = t_values\n        elif pd.api.types.is_datetime64_any_dtype(t_values):\n            # Convert to seconds relative to start\n            t_start_timestamp = t_values[0]\n            t_seconds = (t_values - t_start_timestamp) / np.timedelta64(1, \"s\")\n\n            diffs = np.diff(t_seconds)\n            if len(diffs) &gt; 0 and np.allclose(diffs, diffs[0]):\n                Ts = float(diffs[0])\n                t_start = 0.0  # Relative time\n            else:\n                t_vec = t_seconds\n                t_start = 0.0\n        else:\n            # Fallback, maybe just index\n            t_vec = np.arange(len(t_values))\n            Ts = 1.0\n            t_start = 0.0\n    else:\n        # Ts provided\n        if pd.api.types.is_numeric_dtype(t_values):\n            t_start = float(t_values[0])\n        else:\n            t_start = 0.0\n\n    return cls(t=t_vec, Ts=Ts, t_start=t_start, **series_data)\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.generate_prbs","title":"<code>generate_prbs(N, Ts, seed=42)</code>  <code>staticmethod</code>","text":"<p>Generate a Pseudo-Random Binary Sequence (PRBS).</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>Number of samples.</p> required <code>Ts</code> <code>float</code> <p>Sampling time.</p> required <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (time vector, PRBS signal).</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>@staticmethod\ndef generate_prbs(N: int, Ts: float, seed: int = 42) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate a Pseudo-Random Binary Sequence (PRBS).\n\n    Args:\n        N: Number of samples.\n        Ts: Sampling time.\n        seed: Random seed.\n\n    Returns:\n        Tuple of (time vector, PRBS signal).\n    \"\"\"\n    t = np.linspace(0, Ts * N, num=N, endpoint=False)\n    u = _math.generate_prbs_sequence(N, seed)\n    return t, u\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.lowpass","title":"<code>lowpass(order, corner_frequency, inplace=True)</code>","text":"<p>Apply a Butterworth lowpass filter to all data series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>The order of the filter.</p> required <code>corner_frequency</code> <code>float</code> <p>The corner frequency in Hz. Must be less than the Nyquist frequency.</p> required <code>inplace</code> <code>bool</code> <p>If True, modify in-place. If False, return a copy.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SysIdData</code> <code>SysIdData</code> <p>The filtered object (self if inplace=True, copy if inplace=False).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Ts is not defined or corner_frequency &gt;= Nyquist frequency.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def lowpass(self, order: int, corner_frequency: float, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Apply a Butterworth lowpass filter to all data series.\n\n    Args:\n        order: The order of the filter.\n        corner_frequency: The corner frequency in Hz. Must be less than the Nyquist frequency.\n        inplace: If True, modify in-place. If False, return a copy.\n\n    Returns:\n        SysIdData: The filtered object (self if inplace=True, copy if inplace=False).\n\n    Raises:\n        ValueError: If Ts is not defined or corner_frequency &gt;= Nyquist frequency.\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n    if target.Ts is None:\n        raise ValueError(\"Sampling time 'Ts' is required for filtering.\")\n\n    # Calculate Nyquist frequency and validate corner_frequency\n    nyquist = 1.0 / (2 * target.Ts)\n    if corner_frequency &gt;= nyquist:\n        raise ValueError(\n            f\"Corner frequency ({corner_frequency:.2f} Hz) must be less than Nyquist frequency \"\n            f\"({nyquist:.2f} Hz). Increase sampling rate (decrease Ts={target.Ts}) or use a lower corner frequency. \"\n            f\"Required Ts &lt; {1.0 / (2 * corner_frequency):.4f}s\"\n        )\n\n    sos = scipy.signal.butter(order, corner_frequency, \"low\", analog=False, fs=1.0 / target.Ts, output=\"sos\")\n    for k, _ in target:\n        target.series[k] = scipy.signal.sosfilt(sos, target.series[k])\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.plot","title":"<code>plot()</code>","text":"<p>Plot the data.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def plot(self) -&gt; None:\n    \"\"\"Plot the data.\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        raise ImportError(\n            \"matplotlib is required for plotting. Install it with 'pip install llsi[plot]'.\"\n        ) from None\n\n    t = self.time\n    n_series = len(self.series)\n\n    fig, axes = plt.subplots(n_series, 1, sharex=True, figsize=(10, 2 * n_series))\n    if n_series == 1:\n        axes = [axes]\n\n    for ax, (key, val) in zip(axes, self):\n        ax.plot(t, val, label=key)\n        ax.legend()\n        ax.grid(True)\n\n    plt.xlabel(\"Time\")\n    plt.show()\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.remove","title":"<code>remove(key)</code>","text":"<p>Remove a time series and return self for chaining.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def remove(self, key: str) -&gt; \"SysIdData\":\n    \"\"\"Remove a time series and return self for chaining.\"\"\"\n    del self.series[key]\n    return self\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.resample","title":"<code>resample(factor, inplace=True)</code>","text":"<p>Resample the data using Fourier method (preserves frequency content).</p> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>float</code> <p>Resampling factor. &gt;1 upsamples, &lt;1 downsamples.</p> required Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def resample(self, factor: float, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Resample the data using Fourier method (preserves frequency content).\n\n    Args:\n        factor: Resampling factor. &gt;1 upsamples, &lt;1 downsamples.\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n    N_new = int(target.N * factor)\n    for k, v in target:\n        target.series[k] = scipy.signal.resample(v, N_new)\n\n    if target.t is not None:\n        target.t = scipy.signal.resample(target.t, N_new)\n    else:\n        target.Ts = target.Ts / factor\n\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.split","title":"<code>split(proportion=None, sample=None)</code>","text":"<p>Split the data into two sets.</p> <p>Parameters:</p> Name Type Description Default <code>proportion</code> <code>Optional[float]</code> <p>Ratio of the first set (0 to 1). Default 0.5.</p> <code>None</code> <code>sample</code> <code>Optional[int]</code> <p>Index to split at. Overrides proportion.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[SysIdData, SysIdData]</code> <p>A tuple containing two SysIdData objects.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def split(\n    self, proportion: Optional[float] = None, sample: Optional[int] = None\n) -&gt; Tuple[\"SysIdData\", \"SysIdData\"]:\n    \"\"\"\n    Split the data into two sets.\n\n    Args:\n        proportion: Ratio of the first set (0 to 1). Default 0.5.\n        sample: Index to split at. Overrides proportion.\n\n    Returns:\n        A tuple containing two SysIdData objects.\n    \"\"\"\n    if not sample and not proportion:\n        proportion = 0.5\n\n    if sample is None and proportion is not None:\n        sample = int(round(self.N * proportion))\n\n    # print(f\"Splitting at {sample}\")\n\n    d1 = copy.deepcopy(self)\n    d1.crop(end=sample, inplace=True)\n    d2 = copy.deepcopy(self)\n    d2.crop(start=sample, inplace=True)\n\n    return d1, d2\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.standardize","title":"<code>standardize(inplace=True)</code>","text":"<p>Remove mean and scale to unit variance (Z-score normalization).</p> <p>Stores both means and standard deviations for later unscaling.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def standardize(self, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Remove mean and scale to unit variance (Z-score normalization).\n\n    Stores both means and standard deviations for later unscaling.\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n\n    # First center (this fills target.means)\n    target.center(inplace=True)\n\n    # Then scale to unit variance\n    for k, v in target:\n        sigma = np.std(v)\n        if sigma &lt; 1e-12:  # Protect against division by zero for constant signals\n            sigma = 1.0\n            warnings.warn(f\"Series '{k}' is constant. Skipping scaling.\", stacklevel=2)\n\n        target.series[k] = v / sigma\n        # Store the scaling factor (multiplicatively)\n        target.stds[k] = target.stds.get(k, 1.0) * sigma\n\n    return target\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Convert to pandas DataFrame.</p> <p>Returns:</p> Type Description <code>Any</code> <p>pandas.DataFrame: The data as a DataFrame.</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def to_pandas(self) -&gt; Any:\n    \"\"\"\n    Convert to pandas DataFrame.\n\n    Returns:\n        pandas.DataFrame: The data as a DataFrame.\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for this method. Install it with 'pip install llsi[data]'.\") from None\n\n    df = pd.DataFrame(self.series)\n    df.index = self.time\n    return df\n</code></pre>"},{"location":"api/sysiddata/#llsi.sysiddata.SysIdData.unscale","title":"<code>unscale(inplace=True)</code>","text":"<p>Reverse center() and standardize() transformations (back to physical units).</p> Source code in <code>src/llsi/sysiddata.py</code> <pre><code>def unscale(self, inplace: bool = True) -&gt; \"SysIdData\":\n    \"\"\"\n    Reverse center() and standardize() transformations (back to physical units).\n    \"\"\"\n    target = self if inplace else copy.deepcopy(self)\n\n    # 1. Reverse scaling (multiply by stored stds)\n    if target.stds:\n        for k, v in target:\n            if k in target.stds:\n                target.series[k] = v * target.stds[k]\n        target.stds.clear()  # Reset\n\n    # 2. Reverse centering (add back stored means)\n    if target.means:\n        for k, v in target:\n            if k in target.means:\n                target.series[k] = v + target.means[k]\n        target.means.clear()  # Reset\n\n    return target\n</code></pre>"},{"location":"api/utils/","title":"Utilities","text":""},{"location":"api/utils/#llsi.utils","title":"<code>llsi.utils</code>","text":"<p>Utility functions for system identification.</p>"},{"location":"api/utils/#llsi.utils.cv","title":"<code>cv(training_data, validation_data, y_name, u_name, order, method=None, settings=None, bounds=(0, 100))</code>","text":"<p>Perform cross-validation to find the optimal regularization parameter (lambda).</p> <p>This function optimizes the <code>lambda</code> setting for a system identification method by minimizing the negative fit score on a validation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>training_data</code> <code>SysIdData</code> <p>The dataset used for training the model.</p> required <code>validation_data</code> <code>SysIdData</code> <p>The dataset used for validating the model.</p> required <code>y_name</code> <code>Union[str, List[str]]</code> <p>The name(s) of the output channel(s).</p> required <code>u_name</code> <code>Union[str, List[str]]</code> <p>The name(s) of the input channel(s).</p> required <code>order</code> <code>Any</code> <p>The order of the model to identify.</p> required <code>method</code> <code>Optional[str]</code> <p>The identification method to use (e.g., 'n4sid', 'arx').</p> <code>None</code> <code>settings</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of base settings for the identification method.</p> <code>None</code> <code>bounds</code> <code>Tuple[float, float]</code> <p>A tuple (min, max) specifying the search range for lambda.</p> <code>(0, 100)</code> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: A tuple containing: - The optimal lambda value. - The corresponding fit score (higher is better).</p> Source code in <code>src/llsi/utils.py</code> <pre><code>def cv(\n    training_data: SysIdData,\n    validation_data: SysIdData,\n    y_name: Union[str, List[str]],\n    u_name: Union[str, List[str]],\n    order: Any,\n    method: Optional[str] = None,\n    settings: Optional[Dict[str, Any]] = None,\n    bounds: Tuple[float, float] = (0, 100),\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Perform cross-validation to find the optimal regularization parameter (lambda).\n\n    This function optimizes the `lambda` setting for a system identification method\n    by minimizing the negative fit score on a validation dataset.\n\n    Args:\n        training_data: The dataset used for training the model.\n        validation_data: The dataset used for validating the model.\n        y_name: The name(s) of the output channel(s).\n        u_name: The name(s) of the input channel(s).\n        order: The order of the model to identify.\n        method: The identification method to use (e.g., 'n4sid', 'arx').\n        settings: A dictionary of base settings for the identification method.\n        bounds: A tuple (min, max) specifying the search range for lambda.\n\n    Returns:\n        Tuple[float, float]: A tuple containing:\n            - The optimal lambda value.\n            - The corresponding fit score (higher is better).\n    \"\"\"\n    # Import locally to avoid circular dependencies if any\n    from .sysidalg import sysid\n\n    if settings is None:\n        settings = {}\n\n    def fun(lmb_exp: float) -&gt; float:\n        s = settings.copy()\n        s[\"lambda\"] = 10**lmb_exp\n        mod = sysid(training_data, y_name, u_name, order, method=method, settings=s)\n\n        # Check that required series exist in validation data\n        y_names_list = [y_name] if isinstance(y_name, str) else y_name\n        u_names_list = [u_name] if isinstance(u_name, str) else u_name\n        missing_outputs = [name for name in y_names_list if name not in validation_data]\n        missing_inputs = [name for name in u_names_list if name not in validation_data]\n        if missing_outputs:\n            raise ValueError(f\"Output series not found in validation data: {missing_outputs}\")\n        if missing_inputs:\n            raise ValueError(f\"Input series not found in validation data: {missing_inputs}\")\n\n        y = validation_data[y_name]\n        u = validation_data[u_name]\n        fit = mod.compare(y, u)\n        # print(f\"lambda={10**lmb_exp:.4e}, fit={fit:.4f}\")\n        return -fit\n\n    # Ensure bounds are positive for log\n    lower_bound = max(bounds[0], 1e-12)\n    upper_bound = max(bounds[1], 1e-12)\n\n    bounds_log = (np.log10(lower_bound), np.log10(upper_bound))\n\n    res = scipy.optimize.minimize_scalar(fun, bounds=bounds_log, method=\"bounded\")\n\n    return 10**res.x, -res.fun\n</code></pre>"},{"location":"api/utils/#llsi.utils.load_model","title":"<code>load_model(filename)</code>","text":"<p>Load an LTI model from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the file.</p> required <p>Returns:</p> Name Type Description <code>LTIModel</code> <code>LTIModel</code> <p>The loaded model.</p> Source code in <code>src/llsi/utils.py</code> <pre><code>def load_model(filename: str) -&gt; LTIModel:\n    \"\"\"\n    Load an LTI model from a JSON file.\n\n    Args:\n        filename: The path to the file.\n\n    Returns:\n        LTIModel: The loaded model.\n    \"\"\"\n    import json\n\n    from .polynomialmodel import PolynomialModel\n    from .statespacemodel import StateSpaceModel\n\n    with open(filename) as f:\n        data = json.load(f)\n\n    class_name = data.get(\"__class__\")\n\n    if class_name == \"StateSpaceModel\":\n        return StateSpaceModel.from_json(filename)\n    elif class_name == \"PolynomialModel\":\n        return PolynomialModel.from_json(filename)\n    else:\n        # Fallback: try to infer from keys\n        if \"A\" in data and \"B\" in data:\n            return StateSpaceModel.from_json(filename)\n        elif \"a\" in data and \"b\" in data:\n            return PolynomialModel.from_json(filename)\n        else:\n            raise ValueError(f\"Unknown model type in {filename}\")\n</code></pre>"},{"location":"api/utils/#llsi.utils.rise_time","title":"<code>rise_time(mod, N=200)</code>","text":"<p>Calculate the rise time (10% to 90%) of the step response.</p> <p>Parameters:</p> Name Type Description Default <code>mod</code> <code>LTIModel</code> <p>The LTI model.</p> required <code>N</code> <code>int</code> <p>Number of simulation steps.</p> <code>200</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The rise time in seconds.</p> Source code in <code>src/llsi/utils.py</code> <pre><code>def rise_time(mod: LTIModel, N: int = 200) -&gt; float:\n    \"\"\"\n    Calculate the rise time (10% to 90%) of the step response.\n\n    Args:\n        mod: The LTI model.\n        N: Number of simulation steps.\n\n    Returns:\n        float: The rise time in seconds.\n    \"\"\"\n    t, s = mod.step_response(N=N)\n\n    # Normalize step response if final value is not 1?\n    # Usually rise time is defined relative to steady state value.\n    # Assuming steady state is reached and is non-zero.\n    final_val = s[-1, 0] if s.ndim &gt; 1 else s[-1]\n    if np.abs(final_val) &lt; 1e-6:\n        return 0.0  # Or raise error\n\n    s_norm = s / final_val\n\n    # Find indices\n    # argwhere returns indices where condition is true\n    idx_10 = np.argwhere(s_norm &gt; 0.1)\n    idx_90 = np.argwhere(s_norm &gt; 0.9)\n\n    if len(idx_10) == 0 or len(idx_90) == 0:\n        return float(\"nan\")\n\n    t_10 = t[idx_10[0][0]]\n    t_90 = t[idx_90[0][0]]\n\n    return float(t_90 - t_10)\n</code></pre>"},{"location":"api/utils/#llsi.utils.save_model","title":"<code>save_model(model, filename)</code>","text":"<p>Save an LTI model to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LTIModel</code> <p>The model to save.</p> required <code>filename</code> <code>str</code> <p>The path to the file.</p> required Source code in <code>src/llsi/utils.py</code> <pre><code>def save_model(model: LTIModel, filename: str) -&gt; None:\n    \"\"\"\n    Save an LTI model to a JSON file.\n\n    Args:\n        model: The model to save.\n        filename: The path to the file.\n    \"\"\"\n    if hasattr(model, \"to_json\"):\n        # Add type info to the JSON so we know which class to load\n        import json\n\n        # Get the JSON string from the model\n        json_str = model.to_json()\n        data = json.loads(json_str)\n\n        # Add class name\n        data[\"__class__\"] = model.__class__.__name__\n\n        with open(filename, \"w\") as f:\n            json.dump(data, f, indent=4)\n    else:\n        raise NotImplementedError(f\"Model type {type(model)} does not support to_json serialization.\")\n</code></pre>"},{"location":"api/utils/#llsi.utils.settling_time","title":"<code>settling_time(mod, margin=0.01, N=200)</code>","text":"<p>Calculate the settling time.</p> <p>Parameters:</p> Name Type Description Default <code>mod</code> <code>LTIModel</code> <p>The LTI model.</p> required <code>margin</code> <code>float</code> <p>The error margin (default 1%).</p> <code>0.01</code> <code>N</code> <code>int</code> <p>Number of simulation steps.</p> <code>200</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The settling time in seconds.</p> Source code in <code>src/llsi/utils.py</code> <pre><code>def settling_time(mod: LTIModel, margin: float = 0.01, N: int = 200) -&gt; float:\n    \"\"\"\n    Calculate the settling time.\n\n    Args:\n        mod: The LTI model.\n        margin: The error margin (default 1%).\n        N: Number of simulation steps.\n\n    Returns:\n        float: The settling time in seconds.\n    \"\"\"\n    t, s = mod.step_response(N=N)\n\n    final_val = s[-1, 0] if s.ndim &gt; 1 else s[-1]\n    if np.abs(final_val) &lt; 1e-6:\n        return 0.0\n\n    s_norm = s / final_val\n\n    upper = 1.0 + margin\n    lower = 1.0 - margin\n\n    # Find last time it was outside the margin\n    outside_indices = np.argwhere((s_norm &gt; upper) | (s_norm &lt; lower))\n\n    if len(outside_indices) == 0:\n        return 0.0  # Always inside?\n\n    last_idx = outside_indices[-1][0]\n\n    # Settling time is the time of the last sample outside the margin?\n    # Or the next sample? Usually the time after which it stays inside.\n    # So t[last_idx] is the last time it was bad. t[last_idx+1] is good.\n    # Let's return t[last_idx] as approximation.\n\n    return float(t[last_idx])\n</code></pre>"},{"location":"tutorials/advanced/","title":"Advanced Usage","text":""},{"location":"tutorials/advanced/#residual-analysis","title":"Residual Analysis","text":"<pre><code>with llsi.Figure() as fig:\n    fig.plot({\"mod\": mod, \"data\": val_data}, \"residuals\")\n</code></pre>"},{"location":"tutorials/advanced/#input-delay","title":"Input Delay","text":"<pre><code>mod = llsi.sysid(data, 'y', 'u', method='n4sid', settings={'nk': 5})\n</code></pre>"},{"location":"tutorials/getting_started/","title":"Getting Started","text":""},{"location":"tutorials/getting_started/#installation","title":"Installation","text":"<pre><code>pip install llsi\n</code></pre>"},{"location":"tutorials/getting_started/#basic-usage","title":"Basic Usage","text":"<pre><code>import llsi\nimport numpy as np\n\n# Generate data\ndata = llsi.SysIdData(t=np.arange(100), u=np.random.randn(100), y=np.random.randn(100))\n\n# Identify model\nmod = llsi.sysid(data, 'y', 'u', method='n4sid')\n\n# Plot\nwith llsi.Figure() as fig:\n    fig.plot(mod)\n</code></pre>"}]}